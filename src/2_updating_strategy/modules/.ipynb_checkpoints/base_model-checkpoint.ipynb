{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyomo.environ import *\n",
    "\n",
    "\n",
    "class RawData:\n",
    "    \"Load raw data to be used in model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir, seed=10):\n",
    "        \n",
    "        # Paths to directories\n",
    "        # --------------------\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        \n",
    "        # Network data\n",
    "        # ------------\n",
    "        # Nodes\n",
    "        self.df_n = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_nodes.csv'), index_col='NODE_ID')\n",
    "\n",
    "        # AC edges\n",
    "        self.df_e = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_edges.csv'), index_col='LINE_ID')\n",
    "\n",
    "        # HVDC links\n",
    "        self.df_hvdc_links = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_hvdc_links.csv'), index_col='HVDC_LINK_ID')\n",
    "\n",
    "        # AC interconnector links\n",
    "        self.df_ac_i_links = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_ac_interconnector_links.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "        # AC interconnector flow limits\n",
    "        self.df_ac_i_limits = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_ac_interconnector_flow_limits.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        # ----------       \n",
    "        # Generating unit information\n",
    "        self.df_g = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'generators', 'generators.csv'), index_col='DUID', dtype={'NODE': int})\n",
    "        \n",
    "        # Perturb short-run marginal costs (SRMCs) so all unique \n",
    "        # (add uniformly distributed random number between 0 and 2 to each SRMC. Set seed so this randomness\n",
    "        # can be reproduced)\n",
    "        np.random.seed(seed)\n",
    "        self.df_g['SRMC_2016-17'] = self.df_g['SRMC_2016-17'] + np.random.uniform(0, 2, self.df_g.shape[0])\n",
    "        \n",
    "        \n",
    "        # Load scenario data\n",
    "        # ------------------\n",
    "        with open(os.path.join(scenarios_dir, 'weekly_scenarios.pickle'), 'rb') as f:\n",
    "            self.df_scenarios = pickle.load(f)\n",
    "        \n",
    "\n",
    "class OrganiseData(RawData):\n",
    "    \"Organise data to be used in mathematical program\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        \n",
    "    def get_admittance_matrix(self):\n",
    "        \"Construct admittance matrix for network\"\n",
    "\n",
    "        # Initialise dataframe\n",
    "        df_Y = pd.DataFrame(data=0j, index=self.df_n.index, columns=self.df_n.index)\n",
    "\n",
    "        # Off-diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, tn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, fn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "\n",
    "        # Diagonal elements\n",
    "        for i in self.df_n.index:\n",
    "            df_Y.loc[i, i] = - df_Y.loc[i, :].sum()\n",
    "\n",
    "        # Add shunt susceptance to diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, fn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, tn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "\n",
    "        return df_Y\n",
    "    \n",
    "    \n",
    "    def get_HVDC_incidence_matrix(self):\n",
    "        \"Incidence matrix for HVDC links\"\n",
    "        \n",
    "        # Incidence matrix for HVDC links\n",
    "        df = pd.DataFrame(index=self.df_n.index, columns=self.df_hvdc_links.index, data=0)\n",
    "\n",
    "        for index, row in self.df_hvdc_links.iterrows():\n",
    "            # From nodes assigned a value of 1\n",
    "            df.loc[row['FROM_NODE'], index] = 1\n",
    "\n",
    "            # To nodes assigned a value of -1\n",
    "            df.loc[row['TO_NODE'], index] = -1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_all_ac_edges(self):\n",
    "        \"Tuples defining from and to nodes for all AC edges (forward and reverse)\"\n",
    "        \n",
    "        # Set of all AC edges\n",
    "        edge_set = set()\n",
    "        \n",
    "        # Loop through edges, add forward and reverse direction indice tuples to set\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            edge_set.add((row['FROM_NODE'], row['TO_NODE']))\n",
    "            edge_set.add((row['TO_NODE'], row['FROM_NODE']))\n",
    "        \n",
    "        return edge_set\n",
    "    \n",
    "    \n",
    "    def get_network_graph(self):\n",
    "        \"Graph containing connections between all network nodes\"\n",
    "        network_graph = {n: set() for n in self.df_n.index}\n",
    "\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            network_graph[row['FROM_NODE']].add(row['TO_NODE'])\n",
    "            network_graph[row['TO_NODE']].add(row['FROM_NODE'])\n",
    "        \n",
    "        return network_graph\n",
    "    \n",
    "    \n",
    "    def get_all_dispatchable_fossil_generator_duids(self):\n",
    "        \"Fossil dispatch generator DUIDs\"\n",
    "        \n",
    "        # Filter - keeping only fossil and scheduled generators\n",
    "        mask = (self.df_g['FUEL_CAT'] == 'Fossil') & (self.df_g['SCHEDULE_TYPE'] == 'SCHEDULED')\n",
    "        \n",
    "        return self.df_g[mask].index    \n",
    "       \n",
    "    \n",
    "    def get_reference_nodes(self):\n",
    "        \"Get reference node IDs\"\n",
    "        \n",
    "        # Filter Regional Reference Nodes (RRNs) in Tasmania and Victoria.\n",
    "        mask = (self.df_n['RRN'] == 1) & (self.df_n['NEM_REGION'].isin(['TAS1', 'VIC1']))\n",
    "        reference_node_ids = self.df_n[mask].index\n",
    "        \n",
    "        return reference_node_ids\n",
    "    \n",
    "       \n",
    "    def get_generator_node_map(self, generators):\n",
    "        \"Get set of generators connected to each node\"\n",
    "        \n",
    "        generator_node_map = (self.df_g.reindex(index=generators)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'OMEGA_G': 'DUID'})\n",
    "                              .groupby('NODE').agg(lambda x: set(x))['DUID']\n",
    "                              .reindex(self.df_n.index, fill_value=set()))\n",
    "        \n",
    "        return generator_node_map\n",
    "    \n",
    "    \n",
    "    def get_ac_interconnector_summary(self):\n",
    "        \"Summarise aggregate flow limit information for AC interconnectors\"\n",
    "\n",
    "        # Create dicitionary containing collections of AC branches for which interconnectors are defined. Create\n",
    "        # These collections for both forward and reverse directions.\n",
    "        interconnector_limits = {}\n",
    "\n",
    "        for index, row in self.df_ac_i_limits.iterrows():\n",
    "            # Forward limit\n",
    "            interconnector_limits[index+'-FORWARD'] = {'FROM_REGION': row['FROM_REGION'], 'TO_REGION': row['TO_REGION'], 'LIMIT': row['FORWARD_LIMIT_MW']}\n",
    "\n",
    "            # Reverse limit\n",
    "            interconnector_limits[index+'-REVERSE'] = {'FROM_REGION': row['TO_REGION'], 'TO_REGION': row['FROM_REGION'], 'LIMIT': row['REVERSE_LIMIT_MW']}\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_interconnector_limits = pd.DataFrame(interconnector_limits).T\n",
    "\n",
    "        # Find all branches that consitute each interconnector - order is important. \n",
    "        # First element is 'from' node, second is 'to node\n",
    "        branch_collections = {b: {'branches': list()} for b in df_interconnector_limits.index}\n",
    "\n",
    "        for index, row in self.df_ac_i_links.iterrows():\n",
    "            # For a given branch, find the interconnector index to which it belongs. This will either be the forward or\n",
    "            # reverse direction as defined in the interconnector links DataFrame. If the forward direction, 'FROM_REGION'\n",
    "            # will match between DataFrames, else it indicates the link is in the reverse direction.\n",
    "\n",
    "            # Assign branch to forward interconnector limit ID\n",
    "            mask_forward = (df_interconnector_limits.index.str.contains(index) \n",
    "                      & (df_interconnector_limits['FROM_REGION'] == row['FROM_REGION']) \n",
    "                      & (df_interconnector_limits['TO_REGION'] == row['TO_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_forward = df_interconnector_limits.loc[mask_forward].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_forward]['branches'].append((row['FROM_NODE'], row['TO_NODE']))\n",
    "\n",
    "            # Assign branch to reverse interconnector limit ID\n",
    "            mask_reverse = (df_interconnector_limits.index.str.contains(index) \n",
    "                            & (df_interconnector_limits['FROM_REGION'] == row['TO_REGION']) \n",
    "                            & (df_interconnector_limits['TO_REGION'] == row['FROM_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_reverse = df_interconnector_limits.loc[mask_reverse].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_reverse]['branches'].append((row['TO_NODE'], row['FROM_NODE']))\n",
    "\n",
    "        # Append branch collections to interconnector limits DataFrame\n",
    "        df_interconnector_limits['branches'] = pd.DataFrame(branch_collections).T['branches']\n",
    "        \n",
    "        return df_interconnector_limits\n",
    "\n",
    "    \n",
    "class DCOPFModel(OrganiseData):\n",
    "    \"Create DCOPF model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        # Initialise DCOPF model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        \n",
    "        # Setup solver\n",
    "        # ------------\n",
    "        # Import dual variables\n",
    "        self.model.dual = Suffix(direction=Suffix.IMPORT)\n",
    "        \n",
    "        # Specify solver to be used and output format\n",
    "        self.opt = SolverFactory('gurobi', solver_io='mps')\n",
    "        \n",
    "        \n",
    "        # Parameters used for different scenarios\n",
    "        # ---------------------------------------\n",
    "        # Week index\n",
    "        self.week_index = None\n",
    "        \n",
    "        # Scenario index\n",
    "        self.scenario_index = None\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        \"Create model object\"\n",
    "\n",
    "        # Initialise model\n",
    "        model = ConcreteModel()\n",
    "\n",
    "        # Sets\n",
    "        # ----   \n",
    "        # Nodes\n",
    "        model.OMEGA_N = Set(initialize=self.df_n.index)\n",
    "\n",
    "        # Generators\n",
    "        model.OMEGA_G = Set(initialize=self.get_all_dispatchable_fossil_generator_duids())\n",
    "\n",
    "        # AC edges\n",
    "        ac_edges = self.get_all_ac_edges()\n",
    "        model.OMEGA_NM = Set(initialize=ac_edges)\n",
    "\n",
    "        # Sets of branches for which aggregate AC interconnector limits are defined\n",
    "        ac_limits = self.get_ac_interconnector_summary()\n",
    "        model.OMEGA_J = Set(initialize=ac_limits.index)\n",
    "\n",
    "        # HVDC links\n",
    "        model.OMEGA_H = Set(initialize=self.df_hvdc_links.index)\n",
    "\n",
    "\n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # System base power\n",
    "        model.BASE_POWER = Param(initialize=100)\n",
    "\n",
    "        # Emissions intensity baseline\n",
    "        model.PHI = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Permit price\n",
    "        model.TAU = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Generator emissions intensities\n",
    "        def E_RULE(model, g):\n",
    "            return float(self.df_g.loc[g, 'EMISSIONS'])\n",
    "        model.E = Param(model.OMEGA_G, rule=E_RULE)\n",
    "\n",
    "        # Admittance matrix\n",
    "        admittance_matrix = self.get_admittance_matrix()\n",
    "        def B_RULE(model, n, m):\n",
    "            return float(np.imag(admittance_matrix.loc[n, m]))\n",
    "        model.B = Param(model.OMEGA_NM, rule=B_RULE)\n",
    "\n",
    "        # Reference nodes\n",
    "        reference_nodes = self.get_reference_nodes()\n",
    "        def S_RULE(model, n):\n",
    "            if n in reference_nodes:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        model.S = Param(model.OMEGA_N, rule=S_RULE)\n",
    "\n",
    "        # Generator short-run marginal costs\n",
    "        def C_RULE(model, g):\n",
    "            marginal_cost = float(self.df_g.loc[g, 'SRMC_2016-17'])\n",
    "            return marginal_cost / model.BASE_POWER\n",
    "        model.C = Param(model.OMEGA_G, rule=C_RULE)\n",
    "\n",
    "        # Demand\n",
    "        model.D = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "\n",
    "        # Max voltage angle difference between connected nodes\n",
    "        model.THETA_DELTA = Param(initialize=float(pi / 2))\n",
    "\n",
    "        # HVDC incidence matrix\n",
    "        hvdc_incidence_matrix = self.get_HVDC_incidence_matrix()\n",
    "        def K_RULE(model, n, h):\n",
    "            return float(hvdc_incidence_matrix.loc[n, h])\n",
    "        model.K = Param(model.OMEGA_N, model.OMEGA_H, rule=K_RULE)    \n",
    "\n",
    "        # Aggregate AC interconnector flow limits\n",
    "        def F_RULE(model, j):\n",
    "            power_flow_limit = float(ac_limits.loc[j, 'LIMIT'])\n",
    "            return power_flow_limit / model.BASE_POWER\n",
    "        model.F = Param(model.OMEGA_J, rule=F_RULE)\n",
    "\n",
    "        # Fixed power injections\n",
    "        model.R = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "        \n",
    "        # Maximum power output\n",
    "        def REGISTERED_CAPACITY_RULE(model, g):\n",
    "            registered_capacity = float(self.df_g.loc[g, 'REG_CAP'])\n",
    "            return registered_capacity / model.BASE_POWER\n",
    "        model.REGISTERED_CAPACITY = Param(model.OMEGA_G, rule=REGISTERED_CAPACITY_RULE)\n",
    "\n",
    "        # Generation shock indicator parameter (=0 if generation shock specified, \n",
    "        # else equals 1 if normal operation). Initialize value to 1 (normal operation)\n",
    "        model.GENERATION_SHOCK_FACTOR = Param(model.OMEGA_G, initialize=1, mutable=True)\n",
    "        \n",
    "        # Emissions intensity shock indicator parameter. Used to scale original emissions intensities.\n",
    "        model.EMISSIONS_INTENSITY_SHOCK_FACTOR = Param(model.OMEGA_G, initialize=1, mutable=True)\n",
    "        \n",
    "        \n",
    "        # Variables\n",
    "        # ---------\n",
    "        # Generator output (constrained to non-negative values)\n",
    "        model.p = Var(model.OMEGA_G, within=NonNegativeReals)\n",
    "\n",
    "        # HVDC flows\n",
    "        def P_H_RULE(model, h):\n",
    "            forward_flow_limit = float(self.df_hvdc_links.loc[h, 'FORWARD_LIMIT_MW'])\n",
    "            reverse_flow_limit = float(self.df_hvdc_links.loc[h, 'REVERSE_LIMIT_MW'])\n",
    "            return (- reverse_flow_limit / model.BASE_POWER, forward_flow_limit / model.BASE_POWER)\n",
    "        model.p_H = Var(model.OMEGA_H, bounds=P_H_RULE)\n",
    "\n",
    "        # Node voltage angles\n",
    "        model.theta = Var(model.OMEGA_N)\n",
    "\n",
    "\n",
    "        # Constraints\n",
    "        # -----------\n",
    "        # Power balance\n",
    "        generator_node_map = self.get_generator_node_map(model.OMEGA_G)\n",
    "        network_graph = self.get_network_graph()\n",
    "        def POWER_BALANCE_RULE(model, n):\n",
    "            return (- model.D[n] \n",
    "                    + model.R[n]\n",
    "                    + sum(model.p[g] for g in generator_node_map[n]) \n",
    "                    - sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for m in network_graph[n]) \n",
    "                    - sum(model.K[n, h] * model.p_H[h] for h in model.OMEGA_H) == 0)\n",
    "        model.POWER_BALANCE = Constraint(model.OMEGA_N, rule=POWER_BALANCE_RULE)\n",
    "        \n",
    "        # Max power output\n",
    "        def P_MAX_RULE(model, g):\n",
    "            return model.p[g] <= model.REGISTERED_CAPACITY[g] * model.GENERATION_SHOCK_FACTOR[g]\n",
    "        model.P_MAX = Constraint(model.OMEGA_G, rule=P_MAX_RULE)\n",
    "\n",
    "        # Reference angle\n",
    "        def REFERENCE_ANGLE_RULE(model, n):\n",
    "            if model.S[n] == 1:\n",
    "                return model.theta[n] == 0\n",
    "            else:\n",
    "                return Constraint.Skip\n",
    "        model.REFERENCE_ANGLE = Constraint(model.OMEGA_N, rule=REFERENCE_ANGLE_RULE)\n",
    "\n",
    "        # Voltage angle difference constraint\n",
    "        def VOLTAGE_ANGLE_DIFFERENCE_RULE(model, n, m):\n",
    "            return model.theta[n] - model.theta[m] - model.THETA_DELTA <= 0\n",
    "        model.VOLTAGE_ANGLE_DIFFERENCE = Constraint(model.OMEGA_NM, rule=VOLTAGE_ANGLE_DIFFERENCE_RULE)\n",
    "\n",
    "        # AC interconnector flow constraints\n",
    "        def AC_FLOW_RULE(model, j):\n",
    "            return sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for n, m in ac_limits.loc[j, 'branches'])\n",
    "        model.AC_FLOW = Expression(model.OMEGA_J, rule=AC_FLOW_RULE)\n",
    "\n",
    "        def AC_POWER_FLOW_LIMIT_RULE(model, j):\n",
    "            return model.AC_FLOW[j] - model.F[j] <= 0\n",
    "        model.AC_POWER_FLOW_LIMIT = Constraint(model.OMEGA_J, rule=AC_POWER_FLOW_LIMIT_RULE)\n",
    "\n",
    "        \n",
    "        # Expressions\n",
    "        # -----------\n",
    "        # Effective emissions intensity (original emissions intensity x scaling factor)\n",
    "        def E_HAT_RULE(model, g):\n",
    "            return model.E[g] * model.EMISSIONS_INTENSITY_SHOCK_FACTOR[g]\n",
    "        model.E_HAT = Expression(model.OMEGA_G, rule=E_HAT_RULE)\n",
    "        \n",
    "\n",
    "        # Objective\n",
    "        # ---------\n",
    "        model.OBJECTIVE = Objective(expr=sum((model.C[g] + ((model.E_HAT[g] - model.PHI) * model.TAU)) * model.p[g] for g in model.OMEGA_G))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def update_model_parameters(self, week_index, scenario_index, baseline, permit_price):\n",
    "        \"\"\" Update DCOPF model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : pyomo object\n",
    "            DCOPF OPF model\n",
    "\n",
    "        df_scenarios : pandas DataFrame\n",
    "            Demand and fixed power injection data for each week and each scenario\n",
    "\n",
    "        week_index : int\n",
    "            Index of week for which model should be run\n",
    "\n",
    "        scenario_index : int\n",
    "            Index of scenario that describes operating condition for the given week\n",
    "\n",
    "        baseline: float\n",
    "            Emissions intensity baseline [tCO2/MWh]\n",
    "\n",
    "        permit price : float\n",
    "            Permit price [$/tCO2]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : pyomo object\n",
    "            DCOPF model object with updated parameters.    \n",
    "        \"\"\"\n",
    "\n",
    "        # Update fixed nodal power injections\n",
    "        for n in self.model.OMEGA_N:\n",
    "            self.model.D[n] = float(self.df_scenarios.loc[('demand', n), (week_index, scenario_index)] / self.model.BASE_POWER.value)\n",
    "            self.model.R[n] = float((self.df_scenarios.loc[('hydro', n), (week_index, scenario_index)] + self.df_scenarios.loc[('intermittent', n), (week_index, scenario_index)]) / self.model.BASE_POWER.value)\n",
    "            \n",
    "        # Update emissions intensity baseline\n",
    "        self.model.PHI = float(baseline)\n",
    "\n",
    "        # Update permit price\n",
    "        self.model.TAU = float(permit_price / self.model.BASE_POWER.value)\n",
    "        \n",
    "        # Update week index\n",
    "        self.week_index = week_index\n",
    "        \n",
    "        # Update scenario index\n",
    "        self.scenario_index = scenario_index\n",
    "               \n",
    "            \n",
    "    def solve_model(self):\n",
    "        \"Solve model\"\n",
    "        \n",
    "        self.opt.solve(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_aggregate_weekly_statistics(dcopf_object, scenario_power_output, week_index, baseline, permit_price):\n",
    "    \"\"\"Compute summarised weekly statistics\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dcopf_object : class\n",
    "        Contains DCOPF scenario results and underlying data\n",
    "        \n",
    "    scenario_power_output : dict\n",
    "        power output for each generator for each week and each scenario\n",
    "        \n",
    "    week_index : int\n",
    "        Week for which aggregate statistics are to be calculated\n",
    "        \n",
    "    baseline : int\n",
    "        Emissions intensity baseline that applied for the given week\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    output : dict\n",
    "        Dictionary summarising average emissions intensity of regulated generators, total emissions,\n",
    "        energy output from regulated generators, and net scheme revenue for given week\n",
    "        (dict keys: 'average_regulated_emissions_intensity', 'emissions_tCO2', 'energy_MWh', 'net_revenue')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Power output for a given week\n",
    "    df = pd.DataFrame(scenario_power_output).loc[:, (week_index, slice(None))].reset_index().melt(id_vars=['index']).rename(columns={'variable_0': 'week', 'variable_1': 'scenario', 'value': 'power_pu'}).astype({'week': int, 'scenario': int})\n",
    "\n",
    "    # Scenario durations for a given week\n",
    "    duration = dcopf_object.df_scenarios.loc[('hours', 'duration'), (week_index, slice(None))].reset_index()\n",
    "    duration.columns = duration.columns.droplevel(level=1)\n",
    "\n",
    "    # Merge scenario durations\n",
    "    df = pd.merge(df, duration, how='left', left_on=['week', 'scenario'], right_on=['week', 'scenario'])\n",
    "\n",
    "    # Merge emissions intensity for each DUID\n",
    "#     df = pd.merge(df, dcopf_object.df_g[['EMISSIONS']], how='left', left_on='index', right_index=True)\n",
    "    df['EMISSIONS'] = df.apply(lambda x: dcopf_object.model.E_HAT[x['DUID']], axis=1)\n",
    "    \n",
    "    # Compute total energy output from each generator for each scenario [MWh]\n",
    "    df['energy_MWh'] = df['power_pu'].mul(100).mul(df['hours'])\n",
    "\n",
    "    # Compute total emissions for each scenario [tCO2]\n",
    "    df['emissions_tCO2'] = df['energy_MWh'].mul(df['EMISSIONS'])\n",
    "\n",
    "    # Compute net scheme revenue for each generator for each scenario [$]\n",
    "    df['net_revenue'] = df['EMISSIONS'].subtract(baseline).mul(permit_price).mul(df['energy_MWh'])\n",
    "\n",
    "    # Compute total emissions and total energy output\n",
    "    output = df[['emissions_tCO2', 'energy_MWh', 'net_revenue']].sum().to_dict()\n",
    "\n",
    "    # Average emissions intensity of generators under emissions policy for given week\n",
    "    output['average_regulated_emissions_intensity'] = output['emissions_tCO2'] / output['energy_MWh']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Paths\n",
    "# -----\n",
    "# Directory containing network and generator data\n",
    "data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, os.path.pardir, 'data')\n",
    "\n",
    "# Path to scenarios directory\n",
    "scenarios_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, '1_create_scenarios', 'output')  \n",
    "\n",
    "\n",
    "def run_model(data_dir, scenarios_dir, shock_option, update_mode, update_gain=1, week_of_shock=10, target_scheme_revenue=0, initial_permit_price=40, initial_baseline=1, initial_rolling_scheme_revenue=0, seed=10):\n",
    "    \"\"\"Run model with given parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Path to directory containing core data files used in model initialisation\n",
    "\n",
    "    scenarios_dir : str\n",
    "        Path to directory containing representative operating scenarios\n",
    "\n",
    "    shock_option : str\n",
    "        Specifies type of shock to which system will be subjected. Options\n",
    "\n",
    "        Options:\n",
    "            NO_SHOCKS                 - No shocks to system\n",
    "            GENERATION_SHOCK          - 5% chance that each generator will be unavailable each week \n",
    "                                        beginning from 'week_of_shock'\n",
    "            EMISSIONS_INTENSITY_SHOCK - Emissions intensity scaled by a random number between 0.8 \n",
    "                                        and 1 at 'week_of_shock'\n",
    "    update_mode : str\n",
    "        Specifies how baseline should be updated each week. \n",
    "\n",
    "        Options:\n",
    "            NO_UPDATE         - Same baseline in next iteration\n",
    "            HISTORIC_UPDATE   - Recalibrate baseline assuming next week will be the same as the week just past\n",
    "            HOOKE_UPDATE      - Adjustment is proportional to difference between rolling scheme revenue \n",
    "                                and revenue target\n",
    "        \n",
    "    update_gain : float\n",
    "        Gain used to modify magnitude of restoring force when there is a difference rolling \n",
    "        scheme revenue and the scheme revenue target. Default = 1.\n",
    "\n",
    "    week_of_shock : int\n",
    "        Index for week at which either generation or emissions intensities shocks will be implemented / begin.\n",
    "        Default = 10.\n",
    "\n",
    "    target_scheme_revenue : float\n",
    "        Net scheme revenue target [$]. Default = $0.\n",
    "        \n",
    "    initial_permit_price : float\n",
    "        Initialised permit price value [$/tCO2]. Default = 40 $/tCO2.\n",
    "\n",
    "    initial_baseline : float\n",
    "        Initialised emissions intensity baseline value [tCO2/MWh]. Default = 1 tCO2/MWh.\n",
    "\n",
    "    initial_rolling_scheme_revenue : float\n",
    "        Initialised rolling scheme revenue value [$]. Default = $0.\n",
    "\n",
    "   seed : int\n",
    "        Seed used for random number generator. Allows shocks to be reproduced. Default = 10.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    run_id : str\n",
    "        ID used to identify model run\n",
    "    \"\"\"\n",
    "\n",
    "    # Print model being run\n",
    "    print(f'Running model {update_mode} with run option {shock_option}')\n",
    "\n",
    "    # Summary of all parameters defining the model\n",
    "    parameter_values = (shock_option, update_mode, update_gain, week_of_shock, target_scheme_revenue, initial_permit_price, initial_baseline, initial_rolling_scheme_revenue, seed)\n",
    "\n",
    "    # Convert parameters to string\n",
    "    parameter_values_string = ''.join([str(i) for i in parameter_values])\n",
    "\n",
    "    # Find sha256 of parameter values - used as a unique identifier for the model run\n",
    "    run_id = hashlib.sha256(parameter_values_string.encode('utf-8')).hexdigest()[:8].upper()\n",
    "\n",
    "    # Summary of model options, identified by the hash value of these options\n",
    "    \n",
    "    run_summary = {run_id: {'shock_option': shock_option, 'update_mode': update_mode,\n",
    "                            'update_gain': update_gain, 'week_of_shock': week_of_shock, \n",
    "                            'target_scheme_revenue': target_scheme_revenue,\n",
    "                            'initial_permit_price': initial_permit_price,\n",
    "                            'initial_baseline': initial_baseline, \n",
    "                            'initial_rolling_scheme_revenue': initial_rolling_scheme_revenue, \n",
    "                            'seed': seed}}\n",
    "\n",
    "\n",
    "    # Check if model parameters valid\n",
    "    # -------------------------------\n",
    "    if update_mode not in ['NO_UPDATE', 'HISTORIC_UPDATE', 'HOOKE_UPDATE']:\n",
    "        raise Warning(f'Unexpected update_mode encountered: {update_mode}')\n",
    "\n",
    "    if shock_option not in ['NO_SHOCKS', 'GENERATION_SHOCK', 'EMISSIONS_INTENSITY_SHOCK']:\n",
    "        raise Warning(f'Unexpected shock_option encountered: {shock_option}')\n",
    "\n",
    "\n",
    "    # Create model object\n",
    "    # -------------------\n",
    "    # Instantiate DCOPF model object\n",
    "    DCOPF = DCOPFModel(data_dir=data_dir, scenarios_dir=scenarios_dir)\n",
    "\n",
    "\n",
    "    # Result containers\n",
    "    # -----------------\n",
    "    # Power output for each generator for each scenario\n",
    "    scenario_power_output = dict()\n",
    "\n",
    "    # Prices at each node for each scenario\n",
    "    scenario_nodal_prices = dict()\n",
    "\n",
    "    # Emissions intensity baseline\n",
    "    week_baseline = dict()\n",
    "\n",
    "    # Rolling scheme revenue\n",
    "    week_rolling_scheme_revenue = dict()\n",
    "\n",
    "\n",
    "    # Random shocks \n",
    "    # -------------\n",
    "    # Set seed so random shocks can be reproduced if needed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Specify if generator is on / off for a given week (1=generator is available, 0=generator unavailable)\n",
    "    df_generation_shock_factor = pd.DataFrame(index=DCOPF.model.OMEGA_G, columns=DCOPF.df_scenarios.columns.levels[0], data=1)\n",
    "\n",
    "    # Pick a uniformly distributed random number between 0 and 1. If > 0.95 (i.e. a 5% chance)\n",
    "    # turn generator off for the given week. Only apply generation shocks to weeks after and \n",
    "    # including week_of_shock.\n",
    "    df_generation_shock_factor.loc[:, week_of_shock:] = df_generation_shock_factor.loc[:, week_of_shock:].applymap(lambda x: np.random.uniform(0, 1) if np.random.uniform(0, 1) > 0.95 else 1)\n",
    "\n",
    "    # Augment original emissions intensity by random scaling factor between 0.8 and 1\n",
    "    df_emissions_intensity_shock_factor = pd.Series(index=DCOPF.model.OMEGA_G, data=np.random.uniform(0.8, 1, len(DCOPF.model.OMEGA_G)))\n",
    "\n",
    "\n",
    "    # Run scenarios\n",
    "    # -------------\n",
    "    # For each week\n",
    "    for week_index in DCOPF.df_scenarios.columns.levels[0]:\n",
    "        # Start clock to see how long it takes to solve all scenarios for each week\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Initialise policy parameter if the first week\n",
    "        if week_index == 1:\n",
    "            # Initialise emissions intensity baseline\n",
    "            baseline = initial_baseline\n",
    "            \n",
    "            # Initialise permit price\n",
    "            permit_price = initial_permit_price\n",
    "            \n",
    "            # Initialise rolling scheme revenue\n",
    "            rolling_scheme_revenue = initial_rolling_scheme_revenue\n",
    "            \n",
    "        \n",
    "        # Record baseline applying for the given week\n",
    "        week_baseline[week_index] = baseline\n",
    "\n",
    "        # Apply generation shock if run mode specified, and week index is greater than week_of_shock.\n",
    "        # Note: Different shock occurs each week after week_of_shock.\n",
    "        if shock_option == 'GENERATION_SHOCK':\n",
    "            # Loop through all generators\n",
    "            for g in DCOPF.model.OMEGA_G:\n",
    "                # Initialise all generators to have a state of normal operation for each week\n",
    "                DCOPF.model.GENERATION_SHOCK_FACTOR[g] = float(df_generation_shock_factor.loc[g, week_index])\n",
    "\n",
    "\n",
    "        # Apply emissions intensity shock if run mode specified. Shock occurs once at week index \n",
    "        # given by week_of_shock\n",
    "        elif (shock_option == 'EMISSIONS_INTENSITY_SHOCK') and (week_index == week_of_shock):\n",
    "            # Loop through generators\n",
    "            for g in DCOPF.model.OMEGA_G:\n",
    "                # Augement generator emissions intensity factor\n",
    "                DCOPF.model.EMISSIONS_INTENSITY_SHOCK_FACTOR[g] = float(df_emissions_intensity_shock_factor.loc[g])\n",
    "\n",
    "\n",
    "        # For each representative scenario approximating a week's operating state\n",
    "        for scenario_index in DCOPF.df_scenarios.columns.levels[1]:        \n",
    "            # Update model parameters\n",
    "            DCOPF.update_model_parameters(week_index=week_index, scenario_index=scenario_index, baseline=baseline, permit_price=permit_price)\n",
    "\n",
    "            # Solve model\n",
    "            DCOPF.solve_model()\n",
    "\n",
    "\n",
    "            # Store results\n",
    "            # -------------\n",
    "            # Power output from each generator\n",
    "            scenario_power_output[(week_index, scenario_index)] = DCOPF.model.p.get_values()\n",
    "\n",
    "            # Nodal prices\n",
    "            scenario_nodal_prices[(week_index, scenario_index)] = {n: DCOPF.model.dual[DCOPF.model.POWER_BALANCE[n]] for n in DCOPF.model.OMEGA_N}\n",
    "\n",
    "\n",
    "        # Compute aggregate statistics for week just past\n",
    "        aggregate_weekly_statistics = get_aggregate_weekly_statistics(DCOPF, scenario_power_output, week_index, baseline, permit_price)\n",
    "\n",
    "        # Update rolling scheme revenue\n",
    "        rolling_scheme_revenue += aggregate_weekly_statistics['net_revenue']\n",
    "\n",
    "        # Record rolling scheme revenue\n",
    "        week_rolling_scheme_revenue[week_index] = rolling_scheme_revenue\n",
    "\n",
    "\n",
    "        # Update baseline\n",
    "        # ---------------\n",
    "        if update_mode == 'NO_UPDATE':\n",
    "            # No update to baseline (should be 0 tCO2/MWh)\n",
    "            baseline = baseline\n",
    "\n",
    "        elif update_mode == 'HISTORIC_UPDATE':\n",
    "            # Update baseline based on historic average emissions intensity and total energy output\n",
    "            # (assumes next week will be similar to the week just gone)\n",
    "            baseline += aggregate_weekly_statistics['average_regulated_emissions_intensity'] - update_gain * ((target_scheme_revenue - rolling_scheme_revenue) / (permit_price * aggregate_weekly_statistics['energy_MWh']))\n",
    "            \n",
    "            # Set baseline to 0 if updated value less than zero\n",
    "            if baseline < 0:\n",
    "                baseline = 0\n",
    "            \n",
    "            \n",
    "        elif update_mode == 'HOOKE_UPDATE':\n",
    "            # Increment last week's baseline by an amount proportional to the different \n",
    "            # between rolling scheme revenue and the target, scaled by energy output in the week just past.\n",
    "            baseline += - update_gain * ((target_scheme_revenue - rolling_scheme_revenue) / (permit_price * aggregate_weekly_statistics['energy_MWh']))\n",
    "            \n",
    "            # Set baseline to 0 if updated value less than zero\n",
    "            if baseline < 0:\n",
    "                baseline = 0\n",
    "\n",
    "        print(f'Completed week {week_index} in {time.time()-t0:.2f}s')\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    # ------------\n",
    "    with open(f'output/{run_id}_scenario_nodal_prices.pickle', 'wb') as f:\n",
    "        pickle.dump(scenario_nodal_prices, f)\n",
    "\n",
    "    with open(f'output/{run_id}_scenario_power_output.pickle', 'wb') as f:\n",
    "        pickle.dump(scenario_power_output, f)\n",
    "\n",
    "    with open(f'output/{run_id}_week_baseline.pickle', 'wb') as f:\n",
    "        pickle.dump(week_baseline, f)\n",
    "        \n",
    "    with open(f'output/{run_id}_week_rolling_scheme_revenue.pickle', 'wb') as f:\n",
    "        pickle.dump(week_rolling_scheme_revenue, f)\n",
    "\n",
    "    with open(f'output/{run_id}_run_summary.pickle', 'wb') as f:\n",
    "        pickle.dump(run_summary, f)\n",
    "\n",
    "    with open(f'output/{run_id}_generation_shock_factor.pickle', 'wb') as f:\n",
    "        pickle.dump(df_generation_shock_factor, f)\n",
    "\n",
    "    with open(f'output/{run_id}_emissions_intensity_shock_factor.pickle', 'wb') as f:\n",
    "        pickle.dump(df_emissions_intensity_shock_factor, f)\n",
    "\n",
    "    with open(f'output/{run_id}_generators.pickle', 'wb') as f:\n",
    "        pickle.dump(DCOPF.df_g, f)\n",
    "        \n",
    "        \n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Business-as-usual scenario\n",
    "# bau_id = run_model(data_dir=data_dir,\n",
    "#                    scenarios_dir=scenarios_dir, \n",
    "#                    shock_option='NO_SHOCKS', \n",
    "#                    update_mode='NO_UPDATE', \n",
    "#                    initial_permit_price=0, \n",
    "#                    initial_baseline=0, \n",
    "#                    initial_rolling_scheme_revenue=0)\n",
    "\n",
    "# # Run model with different shocks\n",
    "# for shock_option in ['NO_SHOCKS', 'GENERATION_SHOCK', 'EMISSIONS_INTENSITY_SHOCK']:\n",
    "    \n",
    "#     # Run model with different update methods\n",
    "#     for update_mode in ['NO_UPDATE', 'HOOKE_UPDATE', 'HISTORIC_UPDATE']:\n",
    "#         run_model(data_dir=data_dir, \n",
    "#                   scenarios_dir=scenarios_dir, \n",
    "#                   shock_option=shock_option, \n",
    "#                   update_mode=update_mode, \n",
    "#                   update_gain=1, \n",
    "#                   week_of_shock=10, \n",
    "#                   target_scheme_revenue=0, \n",
    "#                   initial_permit_price=40, \n",
    "#                   initial_baseline=1, \n",
    "#                   initial_rolling_scheme_revenue=0, \n",
    "#                   seed=10)\n",
    "\n",
    "\n",
    "# Specific scenarios\n",
    "# ------------------\n",
    "run_model(data_dir=data_dir, \n",
    "          scenarios_dir=scenarios_dir,\n",
    "          shock_option='EMISSIONS_INTENSITY_SHOCK',\n",
    "          update_mode='HOOKE_UPDATE',\n",
    "          update_gain=1,\n",
    "          week_of_shock=10,\n",
    "          target_scheme_revenue=0,\n",
    "          initial_permit_price=40,\n",
    "          initial_baseline=1,\n",
    "          initial_rolling_scheme_revenue=0,\n",
    "          seed=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti env",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
