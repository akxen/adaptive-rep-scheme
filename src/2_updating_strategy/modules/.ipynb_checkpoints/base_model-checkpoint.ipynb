{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import pandas as pd\n",
    "from pyomo.environ import *\n",
    "\n",
    "\n",
    "class RawData:\n",
    "    \"Load raw data to be used in model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        \n",
    "        # Paths to directories\n",
    "        # --------------------\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        \n",
    "        # Network data\n",
    "        # ------------\n",
    "        # Nodes\n",
    "        self.df_n = pd.read_csv(os.path.join(self.data_dir, 'network_nodes.csv'), index_col='NODE_ID')\n",
    "\n",
    "        # AC edges\n",
    "        self.df_e = pd.read_csv(os.path.join(self.data_dir, 'network_edges.csv'), index_col='LINE_ID')\n",
    "\n",
    "        # HVDC links\n",
    "        self.df_hvdc_links = pd.read_csv(os.path.join(self.data_dir, 'network_hvdc_links.csv'), index_col='HVDC_LINK_ID')\n",
    "\n",
    "        # AC interconnector links\n",
    "        self.df_ac_i_links = pd.read_csv(os.path.join(self.data_dir, 'network_ac_interconnector_links.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "        # AC interconnector flow limits\n",
    "        self.df_ac_i_limits = pd.read_csv(os.path.join(self.data_dir, 'network_ac_interconnector_flow_limits.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        # ----------       \n",
    "        # Generating unit information\n",
    "        self.df_g = pd.read_csv(os.path.join(self.data_dir, 'generators.csv'), index_col='DUID', dtype={'NODE': int})\n",
    "        \n",
    "        # Perturb short-run marginal costs (SRMCs) so all unique \n",
    "        # (add uniformly distributed random number between 0 and 2 to each SRMC)\n",
    "        self.df_g['SRMC_2016-17'] = self.df_g['SRMC_2016-17'] + np.random.uniform(0, 2, self.df_g.shape[0])\n",
    "        \n",
    "        \n",
    "        # Load scenario data\n",
    "        # ------------------\n",
    "        with open(os.path.join(scenarios_dir, 'weekly_scenarios.pickle'), 'rb') as f:\n",
    "            self.df_scenarios = pickle.load(f)\n",
    "        \n",
    "\n",
    "class OrganiseData(RawData):\n",
    "    \"Organise data to be used in mathematical program\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        \n",
    "    def get_admittance_matrix(self):\n",
    "        \"Construct admittance matrix for network\"\n",
    "\n",
    "        # Initialise dataframe\n",
    "        df_Y = pd.DataFrame(data=0j, index=self.df_n.index, columns=self.df_n.index)\n",
    "\n",
    "        # Off-diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, tn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, fn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "\n",
    "        # Diagonal elements\n",
    "        for i in self.df_n.index:\n",
    "            df_Y.loc[i, i] = - df_Y.loc[i, :].sum()\n",
    "\n",
    "        # Add shunt susceptance to diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, fn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, tn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "\n",
    "        return df_Y\n",
    "    \n",
    "    \n",
    "    def get_HVDC_incidence_matrix(self):\n",
    "        \"Incidence matrix for HVDC links\"\n",
    "        \n",
    "        # Incidence matrix for HVDC links\n",
    "        df = pd.DataFrame(index=self.df_n.index, columns=self.df_hvdc_links.index, data=0)\n",
    "\n",
    "        for index, row in self.df_hvdc_links.iterrows():\n",
    "            # From nodes assigned a value of 1\n",
    "            df.loc[row['FROM_NODE'], index] = 1\n",
    "\n",
    "            # To nodes assigned a value of -1\n",
    "            df.loc[row['TO_NODE'], index] = -1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_all_ac_edges(self):\n",
    "        \"Tuples defining from and to nodes for all AC edges (forward and reverse)\"\n",
    "        \n",
    "        # Set of all AC edges\n",
    "        edge_set = set()\n",
    "        \n",
    "        # Loop through edges, add forward and reverse direction indice tuples to set\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            edge_set.add((row['FROM_NODE'], row['TO_NODE']))\n",
    "            edge_set.add((row['TO_NODE'], row['FROM_NODE']))\n",
    "        \n",
    "        return edge_set\n",
    "    \n",
    "    \n",
    "    def get_network_graph(self):\n",
    "        \"Graph containing connections between all network nodes\"\n",
    "        network_graph = {n: set() for n in self.df_n.index}\n",
    "\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            network_graph[row['FROM_NODE']].add(row['TO_NODE'])\n",
    "            network_graph[row['TO_NODE']].add(row['FROM_NODE'])\n",
    "        \n",
    "        return network_graph\n",
    "    \n",
    "    \n",
    "    def get_all_dispatchable_fossil_generator_duids(self):\n",
    "        \"Fossil dispatch generator DUIDs\"\n",
    "        \n",
    "        # Filter - keeping only fossil and scheduled generators\n",
    "        mask = (self.df_g['FUEL_CAT'] == 'Fossil') & (self.df_g['SCHEDULE_TYPE'] == 'SCHEDULED')\n",
    "        \n",
    "        return self.df_g[mask].index    \n",
    "       \n",
    "    \n",
    "    def get_reference_nodes(self):\n",
    "        \"Get reference node IDs\"\n",
    "        \n",
    "        # Filter Regional Reference Nodes (RRNs) in Tasmania and Victoria.\n",
    "        mask = (self.df_n['RRN'] == 1) & (self.df_n['NEM_REGION'].isin(['TAS1', 'VIC1']))\n",
    "        reference_node_ids = self.df_n[mask].index\n",
    "        \n",
    "        return reference_node_ids\n",
    "    \n",
    "       \n",
    "    def get_generator_node_map(self, generators):\n",
    "        \"Get set of generators connected to each node\"\n",
    "        \n",
    "        generator_node_map = (self.df_g.reindex(index=generators)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'OMEGA_G': 'DUID'})\n",
    "                              .groupby('NODE').agg(lambda x: set(x))['DUID']\n",
    "                              .reindex(self.df_n.index, fill_value=set()))\n",
    "        \n",
    "        return generator_node_map\n",
    "    \n",
    "    \n",
    "    def get_ac_interconnector_summary(self):\n",
    "        \"Summarise aggregate flow limit information for AC interconnectors\"\n",
    "\n",
    "        # Create dicitionary containing collections of AC branches for which interconnectors are defined. Create\n",
    "        # These collections for both forward and reverse directions.\n",
    "        interconnector_limits = {}\n",
    "\n",
    "        for index, row in self.df_ac_i_limits.iterrows():\n",
    "            # Forward limit\n",
    "            interconnector_limits[index+'-FORWARD'] = {'FROM_REGION': row['FROM_REGION'], 'TO_REGION': row['TO_REGION'], 'LIMIT': row['FORWARD_LIMIT_MW']}\n",
    "\n",
    "            # Reverse limit\n",
    "            interconnector_limits[index+'-REVERSE'] = {'FROM_REGION': row['TO_REGION'], 'TO_REGION': row['FROM_REGION'], 'LIMIT': row['REVERSE_LIMIT_MW']}\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_interconnector_limits = pd.DataFrame(interconnector_limits).T\n",
    "\n",
    "        # Find all branches that consitute each interconnector - order is important. \n",
    "        # First element is 'from' node, second is 'to node\n",
    "        branch_collections = {b: {'branches': list()} for b in df_interconnector_limits.index}\n",
    "\n",
    "        for index, row in self.df_ac_i_links.iterrows():\n",
    "            # For a given branch, find the interconnector index to which it belongs. This will either be the forward or\n",
    "            # reverse direction as defined in the interconnector links DataFrame. If the forward direction, 'FROM_REGION'\n",
    "            # will match between DataFrames, else it indicates the link is in the reverse direction.\n",
    "\n",
    "            # Assign branch to forward interconnector limit ID\n",
    "            mask_forward = (df_interconnector_limits.index.str.contains(index) \n",
    "                      & (df_interconnector_limits['FROM_REGION'] == row['FROM_REGION']) \n",
    "                      & (df_interconnector_limits['TO_REGION'] == row['TO_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_forward = df_interconnector_limits.loc[mask_forward].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_forward]['branches'].append((row['FROM_NODE'], row['TO_NODE']))\n",
    "\n",
    "            # Assign branch to reverse interconnector limit ID\n",
    "            mask_reverse = (df_interconnector_limits.index.str.contains(index) \n",
    "                            & (df_interconnector_limits['FROM_REGION'] == row['TO_REGION']) \n",
    "                            & (df_interconnector_limits['TO_REGION'] == row['FROM_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_reverse = df_interconnector_limits.loc[mask_reverse].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_reverse]['branches'].append((row['TO_NODE'], row['FROM_NODE']))\n",
    "\n",
    "        # Append branch collections to interconnector limits DataFrame\n",
    "        df_interconnector_limits['branches'] = pd.DataFrame(branch_collections).T['branches']\n",
    "        \n",
    "        return df_interconnector_limits\n",
    "\n",
    "    \n",
    "class DCOPFModel(OrganiseData):\n",
    "    \"Create DCOPF model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        # Initialise DCOPF model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        \n",
    "        # Setup solver\n",
    "        # ------------\n",
    "        # Import dual variables\n",
    "        self.model.dual = Suffix(direction=Suffix.IMPORT)\n",
    "        \n",
    "        # Specify solver to be used and output format\n",
    "        self.opt = SolverFactory('gurobi', solver_io='mps')\n",
    "        self.opt.options['OptimalityTol'] = 1e-9\n",
    "        \n",
    "        \n",
    "        # Parameters used for different scenarios\n",
    "        # ---------------------------------------\n",
    "        # Week index\n",
    "        self.week_index = None\n",
    "        \n",
    "        # Scenario index\n",
    "        self.scenario_index = None\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        \"Create model object\"\n",
    "\n",
    "        # Initialise model\n",
    "        model = ConcreteModel()\n",
    "\n",
    "        # Sets\n",
    "        # ----   \n",
    "        # Nodes\n",
    "        model.OMEGA_N = Set(initialize=self.df_n.index)\n",
    "\n",
    "        # Generators\n",
    "        model.OMEGA_G = Set(initialize=self.get_all_dispatchable_fossil_generator_duids())\n",
    "\n",
    "        # AC edges\n",
    "        ac_edges = self.get_all_ac_edges()\n",
    "        model.OMEGA_NM = Set(initialize=ac_edges)\n",
    "\n",
    "        # Sets of branches for which aggregate AC interconnector limits are defined\n",
    "        ac_limits = self.get_ac_interconnector_summary()\n",
    "        model.OMEGA_J = Set(initialize=ac_limits.index)\n",
    "\n",
    "        # HVDC links\n",
    "        model.OMEGA_H = Set(initialize=self.df_hvdc_links.index)\n",
    "\n",
    "\n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # System base power\n",
    "        model.BASE_POWER = Param(initialize=100)\n",
    "\n",
    "        # Emissions intensity baseline\n",
    "        model.PHI = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Permit price\n",
    "        model.TAU = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Generator emissions intensities\n",
    "        def E_RULE(model, g):\n",
    "            return float(self.df_g.loc[g, 'EMISSIONS'])\n",
    "        model.E = Param(model.OMEGA_G, rule=E_RULE)\n",
    "\n",
    "        # Admittance matrix\n",
    "        admittance_matrix = self.get_admittance_matrix()\n",
    "        def B_RULE(model, n, m):\n",
    "            return float(np.imag(admittance_matrix.loc[n, m]))\n",
    "        model.B = Param(model.OMEGA_NM, rule=B_RULE)\n",
    "\n",
    "        # Reference nodes\n",
    "        reference_nodes = self.get_reference_nodes()\n",
    "        def S_RULE(model, n):\n",
    "            if n in reference_nodes:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        model.S = Param(model.OMEGA_N, rule=S_RULE)\n",
    "\n",
    "        # Generator short-run marginal costs\n",
    "        def C_RULE(model, g):\n",
    "            marginal_cost = float(self.df_g.loc[g, 'SRMC_2016-17'])\n",
    "#             return marginal_cost / model.BASE_POWER\n",
    "            return marginal_cost\n",
    "        model.C = Param(model.OMEGA_G, rule=C_RULE)\n",
    "\n",
    "        # Demand\n",
    "        model.D = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "\n",
    "        # Max voltage angle difference between connected nodes\n",
    "        model.THETA_DELTA = Param(initialize=float(pi / 2))\n",
    "\n",
    "        # HVDC incidence matrix\n",
    "        hvdc_incidence_matrix = self.get_HVDC_incidence_matrix()\n",
    "        def K_RULE(model, n, h):\n",
    "            return float(hvdc_incidence_matrix.loc[n, h])\n",
    "        model.K = Param(model.OMEGA_N, model.OMEGA_H, rule=K_RULE)    \n",
    "\n",
    "        # Aggregate AC interconnector flow limits\n",
    "        def F_RULE(model, j):\n",
    "            power_flow_limit = float(ac_limits.loc[j, 'LIMIT'])\n",
    "            return power_flow_limit / model.BASE_POWER\n",
    "        model.F = Param(model.OMEGA_J, rule=F_RULE)\n",
    "\n",
    "        # Fixed power injections\n",
    "        model.R = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "\n",
    "\n",
    "        # Variables\n",
    "        # ---------\n",
    "        # Generator output\n",
    "        def P_RULE(model, g):\n",
    "            registered_capacity = float(self.df_g.loc[g, 'REG_CAP'])\n",
    "            return (0, registered_capacity / model.BASE_POWER)\n",
    "        model.p = Var(model.OMEGA_G, bounds=P_RULE)\n",
    "\n",
    "        # HVDC flows\n",
    "        def P_H_RULE(model, h):\n",
    "            forward_flow_limit = float(self.df_hvdc_links.loc[h, 'FORWARD_LIMIT_MW'])\n",
    "            reverse_flow_limit = float(self.df_hvdc_links.loc[h, 'REVERSE_LIMIT_MW'])\n",
    "            return (- reverse_flow_limit / model.BASE_POWER, forward_flow_limit / model.BASE_POWER)\n",
    "        model.p_H = Var(model.OMEGA_H, bounds=P_H_RULE)\n",
    "\n",
    "        # Node voltage angles\n",
    "        model.theta = Var(model.OMEGA_N)\n",
    "\n",
    "\n",
    "        # Constraints\n",
    "        # -----------\n",
    "        # Power balance\n",
    "        generator_node_map = self.get_generator_node_map(model.OMEGA_G)\n",
    "        network_graph = self.get_network_graph()\n",
    "        def POWER_BALANCE_RULE(model, n):\n",
    "            return (- model.D[n] \n",
    "                    + model.R[n]\n",
    "                    + sum(model.p[g] for g in generator_node_map[n]) \n",
    "                    - sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for m in network_graph[n]) \n",
    "                    - sum(model.K[n, h] * model.p_H[h] for h in model.OMEGA_H) == 0)\n",
    "        model.POWER_BALANCE = Constraint(model.OMEGA_N, rule=POWER_BALANCE_RULE)\n",
    "\n",
    "        # Reference angle\n",
    "        def REFERENCE_ANGLE_RULE(model, n):\n",
    "            if model.S[n] == 1:\n",
    "                return model.theta[n] == 0\n",
    "            else:\n",
    "                return Constraint.Skip\n",
    "        model.REFERENCE_ANGLE = Constraint(model.OMEGA_N, rule=REFERENCE_ANGLE_RULE)\n",
    "\n",
    "        # Voltage angle difference constraint\n",
    "        def VOLTAGE_ANGLE_DIFFERENCE_RULE(model, n, m):\n",
    "            return model.theta[n] - model.theta[m] - model.THETA_DELTA <= 0\n",
    "        model.VOLTAGE_ANGLE_DIFFERENCE = Constraint(model.OMEGA_NM, rule=VOLTAGE_ANGLE_DIFFERENCE_RULE)\n",
    "\n",
    "        # AC interconnector flow constraints\n",
    "        def AC_FLOW_RULE(model, j):\n",
    "            return sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for n, m in ac_limits.loc[j, 'branches'])\n",
    "        model.AC_FLOW = Expression(model.OMEGA_J, rule=AC_FLOW_RULE)\n",
    "\n",
    "        def AC_POWER_FLOW_LIMIT_RULE(model, j):\n",
    "            return model.AC_FLOW[j] - model.F[j] <= 0\n",
    "        model.AC_POWER_FLOW_LIMIT = Constraint(model.OMEGA_J, rule=AC_POWER_FLOW_LIMIT_RULE)\n",
    "\n",
    "\n",
    "        # Compute absolute flows over HVDC links\n",
    "        # --------------------------------------\n",
    "        model.x_1 = Var(model.OMEGA_H, within=NonNegativeReals)\n",
    "        model.x_2 = Var(model.OMEGA_H, within=NonNegativeReals)\n",
    "\n",
    "        def ABS_HVDC_FLOW_1_RULE(model, h):\n",
    "            return model.x_1[h] >= model.p_H[h]\n",
    "        model.ABS_HVDC_FLOW_1 = Constraint(model.OMEGA_H, rule=ABS_HVDC_FLOW_1_RULE)\n",
    "\n",
    "        def ABS_HVDC_FLOW_2_RULE(model, h):\n",
    "            return model.x_2[h] >= - model.p_H[h]\n",
    "        model.ABS_HVDC_FLOW_2 = Constraint(model.OMEGA_H, rule=ABS_HVDC_FLOW_2_RULE)\n",
    "\n",
    "        def HVDC_FLOW_COST_RULE(model):\n",
    "            return float(0 / model.BASE_POWER)\n",
    "        model.HVDC_FLOW_COST = Param(initialize=HVDC_FLOW_COST_RULE)\n",
    "\n",
    "\n",
    "        # Objective\n",
    "        # ---------\n",
    "        model.OBJECTIVE = Objective(expr=sum((model.C[g] + ((model.E[g] - model.PHI) * model.TAU)) * model.p[g] for g in model.OMEGA_G) + sum((model.x_1[h] + model.x_2[h]) * model.HVDC_FLOW_COST for h in model.OMEGA_H))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def update_model_parameters(self, week_index, scenario_index, baseline, permit_price):\n",
    "        \"\"\" Update DCOPF model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : pyomo object\n",
    "            DCOPF OPF model\n",
    "\n",
    "        df_scenarios : pandas DataFrame\n",
    "            Demand and fixed power injection data for each week and each scenario\n",
    "\n",
    "        week_index : int\n",
    "            Index of week for which model should be run\n",
    "\n",
    "        scenario_index : int\n",
    "            Index of scenario that describes operating condition for the given week\n",
    "\n",
    "        baseline: float\n",
    "            Emissions intensity baseline [tCO2/MWh]\n",
    "\n",
    "        permit price : float\n",
    "            Permit price [$/tCO2]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : pyomo object\n",
    "            DCOPF model object with updated parameters.    \n",
    "        \"\"\"\n",
    "\n",
    "        # Update fixed nodal power injections\n",
    "        for n in self.model.OMEGA_N:\n",
    "            self.model.D[n] = float(self.df_scenarios.loc[('demand', n), (week_index, scenario_index)] / self.model.BASE_POWER.value)\n",
    "            self.model.R[n] = float((self.df_scenarios.loc[('hydro', n), (week_index, scenario_index)] + self.df_scenarios.loc[('intermittent', n), (week_index, scenario_index)]) / self.model.BASE_POWER.value)\n",
    "\n",
    "        # Update emissions intensity baseline\n",
    "        if baseline is not None:\n",
    "            self.model.PHI = float(baseline)\n",
    "\n",
    "        # Update permit price\n",
    "        if permit_price is not None:\n",
    "            self.model.TAU = float(permit_price / self.model.BASE_POWER.value)\n",
    "            \n",
    "        # Update week index\n",
    "        self.week_index = week_index\n",
    "        \n",
    "        # Update scenario index\n",
    "        self.scenario_index = scenario_index\n",
    "               \n",
    "            \n",
    "    def solve_model(self):\n",
    "        \"Solve model\"\n",
    "        \n",
    "        self.opt.solve(self.model)\n",
    "        \n",
    "        \n",
    "    def get_scenario_total_emissions(self):\n",
    "        \"\"\"Total emissions [tCO2] for each scenario\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scenario_emissions : dict\n",
    "            Total emissions for each NEM region and national level for given scenario    \n",
    "        \"\"\"\n",
    "\n",
    "        # Power output and emissions\n",
    "        df = pd.DataFrame({'p': self.model.p.get_values()}).mul(self.model.BASE_POWER.value).join(self.df_g[['EMISSIONS', 'NEM_REGION', 'FUEL_TYPE']], how='left')\n",
    "\n",
    "        # Dictionary in which to store aggregate results\n",
    "        emissions_scenario_results = dict()\n",
    "\n",
    "        # Compute total emissions in each NEM region per hour\n",
    "        df_emissions = df.groupby('NEM_REGION').apply(lambda x: x.prod(axis=1).sum())\n",
    "\n",
    "        # Sum to find national total per hour\n",
    "        df_emissions.loc['NATIONAL'] = df_emissions.sum()\n",
    "\n",
    "        # Multiply by scenario duration to find total emissions for scenario\n",
    "        df_emissions = df_emissions.mul(self.df_scenarios.loc[('hours', 'duration'), (self.week_index, self.scenario_index)])\n",
    "\n",
    "        # Dictionary containing emissions data\n",
    "        scenario_emissions = df_emissions.to_dict()\n",
    "\n",
    "        return scenario_emissions\n",
    "    \n",
    "    \n",
    "    def get_scenario_scheme_revenue(self):\n",
    "        \"\"\"Scheme revenue for given scenario\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scenario_scheme_revenue : dict\n",
    "            Scheme revenue for each NEM region as well as national total for given scenario        \n",
    "        \"\"\"\n",
    "\n",
    "        # Power output and emissions\n",
    "        df = pd.DataFrame({'p': self.model.p.get_values()}).mul(self.model.BASE_POWER.value).join(self.df_g[['EMISSIONS', 'NEM_REGION', 'FUEL_TYPE']], how='left')\n",
    "\n",
    "        # Scheme revenue for each NEM region per hour\n",
    "        df_scheme_revenue = df.groupby('NEM_REGION').apply(lambda x: x['EMISSIONS'].subtract(self.model.PHI.value).mul(x['p']).sum())\n",
    "\n",
    "        # Scheme revenue for nation per hour\n",
    "        df_scheme_revenue.loc['NATIONAL'] = df_scheme_revenue.sum()\n",
    "\n",
    "        # Multiply by scenario duration to get total scheme revenue\n",
    "        df_scheme_revenue = df_scheme_revenue.mul(self.df_scenarios.loc[('hours', 'duration'), (self.week_index, self.scenario_index)])\n",
    "\n",
    "        # Convert to dictionary\n",
    "        scenario_scheme_revenue = df_scheme_revenue.to_dict()\n",
    "        \n",
    "        return scenario_scheme_revenue\n",
    "    \n",
    "    \n",
    "    def get_scenario_energy_revenue(self):\n",
    "        \"\"\"Revenue obtained from energy sales for given scenario [$]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scenario_energy_revenue : dict\n",
    "            Revenue from energy sales for each NEM region as well as national total for given scenario        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Revenue from energy sales\n",
    "        # -------------------------\n",
    "        df = pd.DataFrame.from_dict({n: [self.model.dual[self.model.POWER_BALANCE[n]] * self.model.BASE_POWER.value] for n in self.model.OMEGA_N}, columns=['price'], orient='index')\n",
    "\n",
    "        # Demand for given scenario\n",
    "        df_demand = self.df_scenarios.loc[('demand'), (self.week_index, self.scenario_index)]\n",
    "        df_demand.name = 'demand'\n",
    "\n",
    "        # Total revenue from electricity sales per hour\n",
    "        df_energy_revenue = df.join(df_demand).join(self.df_n['NEM_REGION'], how='left').groupby('NEM_REGION').apply(lambda x: x['price'].mul(x['demand']).sum())\n",
    "        df_energy_revenue.loc['NATIONAL'] = df_energy_revenue.sum()\n",
    "\n",
    "        # Total revenue from energy sales\n",
    "        df_energy_revenue = df_energy_revenue.mul(self.df_scenarios.loc[('hours', 'duration'), (self.week_index, self.scenario_index)])\n",
    "\n",
    "        # Add to energy revenue results dictionary\n",
    "        scenario_energy_revenue = df_energy_revenue.to_dict()\n",
    "        \n",
    "        return scenario_energy_revenue\n",
    "\n",
    "\n",
    "    def get_scenario_generation_by_fuel_type(self):\n",
    "        \"\"\"Generation by fuel type for each NEM region\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scenario_generation_by_fuel_type : dict\n",
    "            Total energy output [MWh] for each type of generating unit for each NEM region as well as national total\n",
    "        \"\"\"\n",
    "\n",
    "        # Power output\n",
    "        df = pd.DataFrame({'p': self.model.p.get_values()}).mul(self.model.BASE_POWER.value).join(self.df_g[['EMISSIONS', 'NEM_REGION', 'FUEL_TYPE']], how='left')\n",
    "\n",
    "        # Energy output by fuel type for each NEM region [MWh]\n",
    "        df_fuel_type_generation = df.groupby(['NEM_REGION', 'FUEL_TYPE'])['p'].sum().mul(self.df_scenarios.loc[('hours', 'duration'), (self.week_index, self.scenario_index)])\n",
    "\n",
    "        # National total\n",
    "        df_nation = df_fuel_type_generation.reset_index().groupby('FUEL_TYPE')['p'].sum().reset_index()\n",
    "        df_nation['NEM_REGION'] = 'NATIONAL'\n",
    "\n",
    "        # Combine regional and national values and convert to dictionary\n",
    "        scenario_generation_by_fuel_type = pd.concat([df_fuel_type_generation.reset_index(), df_nation], sort=False).set_index(['NEM_REGION', 'FUEL_TYPE'])['p'].to_dict()\n",
    "\n",
    "        return scenario_generation_by_fuel_type\n",
    "    \n",
    "    \n",
    "    def get_scenario_energy_output(self):\n",
    "        \"\"\"Energy output from dispatchable generators [MWh] for each NEM region and national total\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        total_energy_output : pandas DataFrame\n",
    "            Total energy output from all dispatchable generators under the emissions policy       \n",
    "        \"\"\"\n",
    "        \n",
    "        # Power output\n",
    "        df = pd.DataFrame({'p': self.model.p.get_values()}).mul(self.model.BASE_POWER.value).join(self.df_g[['NEM_REGION']], how='left')\n",
    "\n",
    "        # Energy output by fuel type for each NEM region [MWh]\n",
    "        df_energy_output = df.groupby(['NEM_REGION'])['p'].sum().mul(self.df_scenarios.loc[('hours', 'duration'), (self.week_index, self.scenario_index)])\n",
    "\n",
    "        # Compute national total\n",
    "        df_energy_output.loc['NATIONAL'] = df_energy_output.sum()\n",
    "\n",
    "        return df_energy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed week 1 in 12.17s\n",
      "Completed week 2 in 12.17s\n",
      "Completed week 3 in 12.30s\n",
      "Completed week 4 in 12.71s\n",
      "Completed week 5 in 12.80s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-75e72d6bef28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mscenario_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDCOPF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_scenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m# Update model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mDCOPF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweek_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweek_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscenario_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscenario_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermit_price\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Solve model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e30a1b34438a>\u001b[0m in \u001b[0;36mupdate_model_parameters\u001b[1;34m(self, week_index, scenario_index, baseline, permit_price)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOMEGA_N\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_scenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'demand'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweek_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscenario_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBASE_POWER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_scenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hydro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweek_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscenario_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_scenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'intermittent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweek_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscenario_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBASE_POWER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;31m# Update emissions intensity baseline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\eee\\Anaconda3\\envs\\opti\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\eee\\Anaconda3\\envs\\opti\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\eee\\Anaconda3\\envs\\opti\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         \u001b[1;31m# we may have a nested tuples indexer here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\eee\\Anaconda3\\envs\\opti\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_nested_tuple_indexer\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_nested_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\eee\\Anaconda3\\envs\\opti\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_nested_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Paths\n",
    "# -----\n",
    "# Directory containing network and generator data\n",
    "data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, os.path.pardir, 'data')\n",
    "\n",
    "# Path to scenarios directory\n",
    "scenarios_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, '1_create_scenarios', 'output')\n",
    "\n",
    "\n",
    "# Instantiate DCOPF model object\n",
    "DCOPF = DCOPFModel(data_dir=data_dir, scenarios_dir=scenarios_dir)\n",
    "\n",
    "\n",
    "# Result containers\n",
    "# -----------------\n",
    "# For total emissions results\n",
    "scenario_total_emissions = dict()\n",
    "\n",
    "# For scheme revenue results\n",
    "scenario_scheme_revenue = dict()\n",
    "\n",
    "# For energy revenue\n",
    "scenario_energy_revenue = dict()\n",
    "\n",
    "# For generation by fuel type\n",
    "scenario_generation_by_fuel_type = dict()\n",
    "\n",
    "# For energy output from generators regulated by the emissions policy\n",
    "scenario_energy_output = dict()\n",
    "\n",
    "# Baseline\n",
    "weekly_baseline = dict()\n",
    "\n",
    "# Rolling scheme revenue\n",
    "rolling_scheme_revenue = dict()\n",
    "\n",
    "\n",
    "# Run scenarios\n",
    "# -------------\n",
    "# Initialise rolling scheme revenue\n",
    "rolling_scheme_revenue = 0\n",
    "\n",
    "# Initialise emissions intensity baseline\n",
    "baseline = 1\n",
    "\n",
    "# Permit price [$/tCO2]\n",
    "permit_price = 40\n",
    "\n",
    "# Target scheme revenue [$] (want scheme to be revenue neutral)\n",
    "target_scheme_revenue = 0\n",
    "\n",
    "\n",
    "# For each week\n",
    "for week_index in DCOPF.df_scenarios.columns.levels[0]:\n",
    "    # Start clock to see how long it takes to solve all scenarios for each week\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Record baseline for coming scenario\n",
    "    weekly_baseline[week_index] = baseline\n",
    "       \n",
    "    # For each representative scenario approximating a week's operating state\n",
    "    for scenario_index in DCOPF.df_scenarios.columns.levels[1]:        \n",
    "        # Update model parameters\n",
    "        DCOPF.update_model_parameters(week_index=week_index, scenario_index=scenario_index, baseline=baseline, permit_price=40)\n",
    "\n",
    "        # Solve model\n",
    "        DCOPF.solve_model()\n",
    "\n",
    "\n",
    "        # Extract results from model object\n",
    "        # ---------------------------------\n",
    "        # Emissions\n",
    "        scenario_total_emissions[(week_index, scenario_index)] = DCOPF.get_scenario_total_emissions()\n",
    "\n",
    "        # Scheme revenue\n",
    "        scenario_scheme_revenue[(week_index, scenario_index)] = DCOPF.get_scenario_scheme_revenue()\n",
    "\n",
    "        # Revenue from energy sales\n",
    "        scenario_energy_revenue[(week_index, scenario_index)] = DCOPF.get_scenario_energy_revenue()\n",
    "\n",
    "        # Generation by fuel type\n",
    "        scenario_generation_by_fuel_type[(week_index, scenario_index)] = DCOPF.get_scenario_generation_by_fuel_type()\n",
    "        \n",
    "        # Energy output\n",
    "        scenario_energy_output[(week_index, scenario_index)] = DCOPF.get_scenario_energy_output()\n",
    "        \n",
    "        \n",
    "        # Record rolling scheme revenue and update baseline\n",
    "        # -------------------------------------------------\n",
    "        # Update rolling scheme revenue\n",
    "        rolling_scheme_revenue += scenario_scheme_revenue[(week_index, scenario_index)]['NATIONAL']\n",
    "\n",
    "    # Total energy output for week just calculated [MWh]\n",
    "    total_energy_output = sum(scenario_energy_output[(week_index, i)]['NATIONAL'] for i in DCOPF.df_scenarios.columns.levels[1])\n",
    "    \n",
    "    # Average emissions intenstiy of generators under emissions policy for week just calculated [tCO2/MWh]\n",
    "    regulated_generators_emissions_intensity = sum(scenario_total_emissions[(week_index, i)]['NATIONAL'] for i in DCOPF.df_scenarios.columns.levels[1]) / total_energy_output\n",
    "\n",
    "    # Update baseline\n",
    "    baseline = regulated_generators_emissions_intensity - ((target_scheme_revenue - rolling_scheme_revenue) / (permit_price * total_energy_output))\n",
    "           \n",
    "    print(f'Completed week {week_index} in {time.time()-t0:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total emissions for each NEM region and nation\n",
    "* Total scheme revenue for each NEM region and nation\n",
    "* Generation by fuel type and NEM region and nation\n",
    "\n",
    "Then aggregate scenarios into weekly values\n",
    "* Average price for each NEM region\n",
    "* Emissions intensity for system and regulated generators, for nation and each NEM region\n",
    "* Generation by fuel type and NEM region and nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ProcessScenarioResults(OrganiseData):\n",
    "    \"Aggregate scenario results to weekly statistics\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        self.df_weekly_fixed_injections = self.get_weekly_fixed_energy_injections()\n",
    "\n",
    "        \n",
    "    def get_weekly_fixed_energy_injections(self):\n",
    "        \"Fixed demand, hydro, and intermittent injections aggregated to weekly level\"\n",
    "\n",
    "        # Fixed injections for NEM regions\n",
    "        df_weekly_fixed_injections_regional = self.df_scenarios.T.drop(('hours', 'duration'), axis=1, level=0).mul(self.df_scenarios.T.loc[:, ('hours', 'duration')], axis=0).reset_index().drop('scenario', axis=1, level=0).sort_index(axis=1).groupby('week').sum().T.join(self.df_n[['NEM_REGION']], how='left').reset_index().drop('NODE_ID', axis=1).groupby(by=['level', 'NEM_REGION']).sum().reset_index()\n",
    "\n",
    "        # Sum to get national total fixed energy injections\n",
    "        df_weekly_fixed_injections_national = df_weekly_fixed_injections_regional.groupby(by=['level']).sum().reset_index()\n",
    "        df_weekly_fixed_injections_national['NEM_REGION'] = 'NATIONAL'\n",
    "\n",
    "        # Concatenate so all data is in same DataFrame\n",
    "        df_weekly_fixed_injections = pd.concat([df_weekly_fixed_injections_regional, df_weekly_fixed_injections_national], sort=False).set_index(['level', 'NEM_REGION']).T\n",
    "\n",
    "        return df_weekly_fixed_injections\n",
    "    \n",
    "    \n",
    "    def get_weekly_average_energy_price(self, scenario_energy_revenue):\n",
    "        \"Compute average prices\"\n",
    "        \n",
    "        # Weekly revenue from energy sales for each NEM region and national total\n",
    "        df_weekly_energy_revenue = pd.DataFrame.from_dict(scenario_energy_revenue, orient='index').reset_index().drop('level_1', axis=1).groupby('level_0').sum()\n",
    "\n",
    "        # Average weekly wholesale price [$/MWh]\n",
    "        df_weekly_average_energy_price = df_weekly_energy_revenue.div(self.df_weekly_fixed_injections.loc[:, 'demand'])\n",
    "        \n",
    "        return df_weekly_average_energy_price\n",
    "    \n",
    "    \n",
    "    def get_weekly_emissions(self, scenario_total_emissions):\n",
    "        \"Total emissions each week\"\n",
    "        \n",
    "        # Total weekly emissions for each NEM zone and national total\n",
    "        df_weekly_emissions = pd.DataFrame.from_dict(scenario_total_emissions, orient='index').reset_index().drop('level_1', axis=1).groupby('level_0').sum()\n",
    "        \n",
    "        return df_weekly_emissions\n",
    "\n",
    "    \n",
    "    def get_weekly_system_emissions_intensity(self, scenario_total_emissions):\n",
    "        \"Average emissions of system each week\"\n",
    "        \n",
    "        # Total weekly emissions for each NEM zone and national total\n",
    "        df_weekly_emissions = self.get_weekly_emissions(scenario_total_emissions)\n",
    "        \n",
    "        # Average system emissions intensity for each each week\n",
    "        df_weekly_system_emissions_intensity = df_weekly_emissions.div(self.df_weekly_fixed_injections.loc[:, 'demand'])\n",
    "\n",
    "        return df_weekly_system_emissions_intensity\n",
    "\n",
    "    \n",
    "    def get_weekly_regulated_generators_emissions_intensity(self, scenario_total_emissions, scenario_generation_by_fuel_type):\n",
    "        \"Average emissions intensity for regulated generators each week\"\n",
    "        \n",
    "        # Total emissions each week\n",
    "        df_weekly_emissions = self.get_weekly_emissions(scenario_total_emissions)\n",
    "        \n",
    "        # Weekly energy output from generators subject to the emissions reduction scheme\n",
    "        df_weekly_regulated_generator_energy_output = pd.DataFrame(scenario_generation_by_fuel_type).T.reset_index().drop('level_1', axis=1, level=0).sort_index(axis=1).groupby('level_0').sum().T.reset_index().drop('level_1', axis=1).groupby('level_0').sum().T\n",
    "\n",
    "        # Average weekly emissions intensity of regulated generators\n",
    "        df_weekly_regulated_generators_emissions_intensity = df_weekly_emissions.div(df_weekly_regulated_generator_energy_output).fillna(0)\n",
    "\n",
    "        return df_weekly_regulated_generators_emissions_intensity\n",
    "\n",
    "    \n",
    "    def get_weekly_generation_by_fuel_type(self, scenario_generation_by_fuel_type):\n",
    "        \"Aggregate weekly energy output by fuel type\"\n",
    "        \n",
    "        # Generation by fuel type - regional and national statistics\n",
    "        df_weekly_generation_by_fuel_type = pd.DataFrame(scenario_generation_by_fuel_type).T.reset_index().sort_index(axis=1).drop('level_1', axis=1).groupby('level_0').sum()\n",
    "        \n",
    "        return df_weekly_generation_by_fuel_type\n",
    "    \n",
    "    \n",
    "# Class object used to process scenario results and produce weekly statistics\n",
    "WeeklyResults = ProcessScenarioResults(data_dir, scenarios_dir)\n",
    "\n",
    "# Average energy price each week - regional and national statistics\n",
    "weekly_average_energy_price = WeeklyResults.get_weekly_average_energy_price(scenario_energy_revenue)\n",
    "\n",
    "# Average system emissions intensity each week - regional and national statistics\n",
    "weekly_system_emissions_intensity = WeeklyResults.get_weekly_system_emissions_intensity(scenario_total_emissions)\n",
    "\n",
    "# Average emissions intensity of generators subject to emissions policy - regional and national statistics\n",
    "weekly_regulated_generators_emissions_intensity = WeeklyResults.get_weekly_regulated_generators_emissions_intensity(scenario_total_emissions, scenario_generation_by_fuel_type)\n",
    "\n",
    "# Generation by fuel type\n",
    "weekly_generation_by_fuel_type = WeeklyResults.get_weekly_generation_by_fuel_type(scenario_generation_by_fuel_type)\n",
    "\n",
    "plt.clf()\n",
    "pd.DataFrame.from_dict({week: {'baseline': baseline} for week, baseline in weekly_baseline.items()}, orient='index').plot()\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "pd.DataFrame.from_dict(scenario_scheme_revenue, orient='index').reset_index().sort_index(axis=1).drop('level_1', axis=1).groupby('level_0').sum().cumsum()['NATIONAL'].plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti env",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
