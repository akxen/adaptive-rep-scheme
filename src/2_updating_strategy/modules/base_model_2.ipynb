{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyomo.environ import *\n",
    "\n",
    "\n",
    "class RawData:\n",
    "    \"Load raw data to be used in model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir, seed=10):\n",
    "        \n",
    "        # Paths to directories\n",
    "        # --------------------\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        \n",
    "        # Network data\n",
    "        # ------------\n",
    "        # Nodes\n",
    "        self.df_n = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_nodes.csv'), index_col='NODE_ID')\n",
    "\n",
    "        # AC edges\n",
    "        self.df_e = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_edges.csv'), index_col='LINE_ID')\n",
    "\n",
    "        # HVDC links\n",
    "        self.df_hvdc_links = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_hvdc_links.csv'), index_col='HVDC_LINK_ID')\n",
    "\n",
    "        # AC interconnector links\n",
    "        self.df_ac_i_links = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_ac_interconnector_links.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "        # AC interconnector flow limits\n",
    "        self.df_ac_i_limits = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'network', 'network_ac_interconnector_flow_limits.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        # ----------       \n",
    "        # Generating unit information\n",
    "        self.df_g = pd.read_csv(os.path.join(self.data_dir, 'egrimod-nem-dataset-v1.3', 'akxen-egrimod-nem-dataset-4806603', 'generators', 'generators.csv'), index_col='DUID', dtype={'NODE': int})\n",
    "        \n",
    "        # Perturb short-run marginal costs (SRMCs) so all unique \n",
    "        # (add uniformly distributed random number between 0 and 2 to each SRMC. Set seed so this randomness\n",
    "        # can be reproduced)\n",
    "        np.random.seed(seed)\n",
    "        self.df_g['SRMC_2016-17'] = self.df_g['SRMC_2016-17'] + np.random.uniform(0, 2, self.df_g.shape[0])\n",
    "        \n",
    "        \n",
    "        # Load scenario data\n",
    "        # ------------------\n",
    "        with open(os.path.join(scenarios_dir, 'weekly_scenarios.pickle'), 'rb') as f:\n",
    "            self.df_scenarios = pickle.load(f)\n",
    "        \n",
    "\n",
    "class OrganiseData(RawData):\n",
    "    \"Organise data to be used in mathematical program\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        \n",
    "    def get_admittance_matrix(self):\n",
    "        \"Construct admittance matrix for network\"\n",
    "\n",
    "        # Initialise dataframe\n",
    "        df_Y = pd.DataFrame(data=0j, index=self.df_n.index, columns=self.df_n.index)\n",
    "\n",
    "        # Off-diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, tn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, fn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "\n",
    "        # Diagonal elements\n",
    "        for i in self.df_n.index:\n",
    "            df_Y.loc[i, i] = - df_Y.loc[i, :].sum()\n",
    "\n",
    "        # Add shunt susceptance to diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, fn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, tn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "\n",
    "        return df_Y\n",
    "    \n",
    "    \n",
    "    def get_HVDC_incidence_matrix(self):\n",
    "        \"Incidence matrix for HVDC links\"\n",
    "        \n",
    "        # Incidence matrix for HVDC links\n",
    "        df = pd.DataFrame(index=self.df_n.index, columns=self.df_hvdc_links.index, data=0)\n",
    "\n",
    "        for index, row in self.df_hvdc_links.iterrows():\n",
    "            # From nodes assigned a value of 1\n",
    "            df.loc[row['FROM_NODE'], index] = 1\n",
    "\n",
    "            # To nodes assigned a value of -1\n",
    "            df.loc[row['TO_NODE'], index] = -1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_all_ac_edges(self):\n",
    "        \"Tuples defining from and to nodes for all AC edges (forward and reverse)\"\n",
    "        \n",
    "        # Set of all AC edges\n",
    "        edge_set = set()\n",
    "        \n",
    "        # Loop through edges, add forward and reverse direction indice tuples to set\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            edge_set.add((row['FROM_NODE'], row['TO_NODE']))\n",
    "            edge_set.add((row['TO_NODE'], row['FROM_NODE']))\n",
    "        \n",
    "        return edge_set\n",
    "    \n",
    "    \n",
    "    def get_network_graph(self):\n",
    "        \"Graph containing connections between all network nodes\"\n",
    "        network_graph = {n: set() for n in self.df_n.index}\n",
    "\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            network_graph[row['FROM_NODE']].add(row['TO_NODE'])\n",
    "            network_graph[row['TO_NODE']].add(row['FROM_NODE'])\n",
    "        \n",
    "        return network_graph\n",
    "    \n",
    "    \n",
    "    def get_all_dispatchable_fossil_generator_duids(self):\n",
    "        \"Fossil dispatch generator DUIDs\"\n",
    "        \n",
    "        # Filter - keeping only fossil and scheduled generators\n",
    "        mask = (self.df_g['FUEL_CAT'] == 'Fossil') & (self.df_g['SCHEDULE_TYPE'] == 'SCHEDULED')\n",
    "        \n",
    "        return self.df_g[mask].index    \n",
    "       \n",
    "    \n",
    "    def get_reference_nodes(self):\n",
    "        \"Get reference node IDs\"\n",
    "        \n",
    "        # Filter Regional Reference Nodes (RRNs) in Tasmania and Victoria.\n",
    "        mask = (self.df_n['RRN'] == 1) & (self.df_n['NEM_REGION'].isin(['TAS1', 'VIC1']))\n",
    "        reference_node_ids = self.df_n[mask].index\n",
    "        \n",
    "        return reference_node_ids\n",
    "    \n",
    "       \n",
    "    def get_generator_node_map(self, generators):\n",
    "        \"Get set of generators connected to each node\"\n",
    "        \n",
    "        generator_node_map = (self.df_g.reindex(index=generators)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'OMEGA_G': 'DUID'})\n",
    "                              .groupby('NODE').agg(lambda x: set(x))['DUID']\n",
    "                              .reindex(self.df_n.index, fill_value=set()))\n",
    "        \n",
    "        return generator_node_map\n",
    "    \n",
    "    \n",
    "    def get_ac_interconnector_summary(self):\n",
    "        \"Summarise aggregate flow limit information for AC interconnectors\"\n",
    "\n",
    "        # Create dicitionary containing collections of AC branches for which interconnectors are defined. Create\n",
    "        # These collections for both forward and reverse directions.\n",
    "        interconnector_limits = {}\n",
    "\n",
    "        for index, row in self.df_ac_i_limits.iterrows():\n",
    "            # Forward limit\n",
    "            interconnector_limits[index+'-FORWARD'] = {'FROM_REGION': row['FROM_REGION'], 'TO_REGION': row['TO_REGION'], 'LIMIT': row['FORWARD_LIMIT_MW']}\n",
    "\n",
    "            # Reverse limit\n",
    "            interconnector_limits[index+'-REVERSE'] = {'FROM_REGION': row['TO_REGION'], 'TO_REGION': row['FROM_REGION'], 'LIMIT': row['REVERSE_LIMIT_MW']}\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df_interconnector_limits = pd.DataFrame(interconnector_limits).T\n",
    "\n",
    "        # Find all branches that consitute each interconnector - order is important. \n",
    "        # First element is 'from' node, second is 'to node\n",
    "        branch_collections = {b: {'branches': list()} for b in df_interconnector_limits.index}\n",
    "\n",
    "        for index, row in self.df_ac_i_links.iterrows():\n",
    "            # For a given branch, find the interconnector index to which it belongs. This will either be the forward or\n",
    "            # reverse direction as defined in the interconnector links DataFrame. If the forward direction, 'FROM_REGION'\n",
    "            # will match between DataFrames, else it indicates the link is in the reverse direction.\n",
    "\n",
    "            # Assign branch to forward interconnector limit ID\n",
    "            mask_forward = (df_interconnector_limits.index.str.contains(index) \n",
    "                      & (df_interconnector_limits['FROM_REGION'] == row['FROM_REGION']) \n",
    "                      & (df_interconnector_limits['TO_REGION'] == row['TO_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_forward = df_interconnector_limits.loc[mask_forward].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_forward]['branches'].append((row['FROM_NODE'], row['TO_NODE']))\n",
    "\n",
    "            # Assign branch to reverse interconnector limit ID\n",
    "            mask_reverse = (df_interconnector_limits.index.str.contains(index) \n",
    "                            & (df_interconnector_limits['FROM_REGION'] == row['TO_REGION']) \n",
    "                            & (df_interconnector_limits['TO_REGION'] == row['FROM_REGION']))\n",
    "\n",
    "            # Interconnector ID corresponding to branch \n",
    "            branch_index_reverse = df_interconnector_limits.loc[mask_reverse].index[0]\n",
    "\n",
    "            # Add branch tuple to branch collection\n",
    "            branch_collections[branch_index_reverse]['branches'].append((row['TO_NODE'], row['FROM_NODE']))\n",
    "\n",
    "        # Append branch collections to interconnector limits DataFrame\n",
    "        df_interconnector_limits['branches'] = pd.DataFrame(branch_collections).T['branches']\n",
    "        \n",
    "        return df_interconnector_limits\n",
    "\n",
    "    \n",
    "class DCOPFModel(OrganiseData):\n",
    "    \"Create DCOPF model\"\n",
    "    \n",
    "    def __init__(self, data_dir, scenarios_dir):\n",
    "        # Load model data\n",
    "        super().__init__(data_dir, scenarios_dir)\n",
    "        \n",
    "        # Initialise DCOPF model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        \n",
    "        # Setup solver\n",
    "        # ------------\n",
    "        # Import dual variables\n",
    "        self.model.dual = Suffix(direction=Suffix.IMPORT)\n",
    "        \n",
    "        # Specify solver to be used and output format\n",
    "        self.opt = SolverFactory('gurobi', solver_io='mps')\n",
    "        \n",
    "        \n",
    "        # Parameters used for different scenarios\n",
    "        # ---------------------------------------\n",
    "        # Week index\n",
    "        self.week_index = None\n",
    "        \n",
    "        # Scenario index\n",
    "        self.scenario_index = None\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        \"Create model object\"\n",
    "\n",
    "        # Initialise model\n",
    "        model = ConcreteModel()\n",
    "\n",
    "        # Sets\n",
    "        # ----   \n",
    "        # Nodes\n",
    "        model.OMEGA_N = Set(initialize=self.df_n.index)\n",
    "\n",
    "        # Generators\n",
    "        model.OMEGA_G = Set(initialize=self.get_all_dispatchable_fossil_generator_duids())\n",
    "\n",
    "        # AC edges\n",
    "        ac_edges = self.get_all_ac_edges()\n",
    "        model.OMEGA_NM = Set(initialize=ac_edges)\n",
    "\n",
    "        # Sets of branches for which aggregate AC interconnector limits are defined\n",
    "        ac_limits = self.get_ac_interconnector_summary()\n",
    "        model.OMEGA_J = Set(initialize=ac_limits.index)\n",
    "\n",
    "        # HVDC links\n",
    "        model.OMEGA_H = Set(initialize=self.df_hvdc_links.index)\n",
    "\n",
    "\n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # System base power\n",
    "        model.BASE_POWER = Param(initialize=100)\n",
    "\n",
    "        # Emissions intensity baseline\n",
    "        model.PHI = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Permit price\n",
    "        model.TAU = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Generator emissions intensities\n",
    "        def E_RULE(model, g):\n",
    "            return float(self.df_g.loc[g, 'EMISSIONS'])\n",
    "        model.E = Param(model.OMEGA_G, rule=E_RULE)\n",
    "\n",
    "        # Admittance matrix\n",
    "        admittance_matrix = self.get_admittance_matrix()\n",
    "        def B_RULE(model, n, m):\n",
    "            return float(np.imag(admittance_matrix.loc[n, m]))\n",
    "        model.B = Param(model.OMEGA_NM, rule=B_RULE)\n",
    "\n",
    "        # Reference nodes\n",
    "        reference_nodes = self.get_reference_nodes()\n",
    "        def S_RULE(model, n):\n",
    "            if n in reference_nodes:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        model.S = Param(model.OMEGA_N, rule=S_RULE)\n",
    "\n",
    "        # Generator short-run marginal costs\n",
    "        def C_RULE(model, g):\n",
    "            marginal_cost = float(self.df_g.loc[g, 'SRMC_2016-17'])\n",
    "            return marginal_cost / model.BASE_POWER\n",
    "        model.C = Param(model.OMEGA_G, rule=C_RULE)\n",
    "\n",
    "        # Demand\n",
    "        model.D = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "\n",
    "        # Max voltage angle difference between connected nodes\n",
    "        model.THETA_DELTA = Param(initialize=float(pi / 2))\n",
    "\n",
    "        # HVDC incidence matrix\n",
    "        hvdc_incidence_matrix = self.get_HVDC_incidence_matrix()\n",
    "        def K_RULE(model, n, h):\n",
    "            return float(hvdc_incidence_matrix.loc[n, h])\n",
    "        model.K = Param(model.OMEGA_N, model.OMEGA_H, rule=K_RULE)    \n",
    "\n",
    "        # Aggregate AC interconnector flow limits\n",
    "        def F_RULE(model, j):\n",
    "            power_flow_limit = float(ac_limits.loc[j, 'LIMIT'])\n",
    "            return power_flow_limit / model.BASE_POWER\n",
    "        model.F = Param(model.OMEGA_J, rule=F_RULE)\n",
    "\n",
    "        # Fixed power injections\n",
    "        model.R = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "        \n",
    "        # Maximum power output\n",
    "        def REGISTERED_CAPACITY_RULE(model, g):\n",
    "            registered_capacity = float(self.df_g.loc[g, 'REG_CAP'])\n",
    "            return registered_capacity / model.BASE_POWER\n",
    "        model.REGISTERED_CAPACITY = Param(model.OMEGA_G, rule=REGISTERED_CAPACITY_RULE)\n",
    "\n",
    "        # Emissions intensity shock indicator parameter. Used to scale original emissions intensities.\n",
    "        model.EMISSIONS_INTENSITY_SHOCK_FACTOR = Param(model.OMEGA_G, initialize=1, mutable=True)\n",
    "        \n",
    "        # Duration of each scenario (will be updated each time model is run. Useful when computing\n",
    "        # total emissions / total scheme revenue etc.)\n",
    "        model.SCENARIO_DURATION = Param(initialize=0, mutable=True)\n",
    "        \n",
    "        \n",
    "        # Variables\n",
    "        # ---------\n",
    "        # Generator output (constrained to non-negative values)\n",
    "        model.p = Var(model.OMEGA_G, within=NonNegativeReals)\n",
    "\n",
    "        # HVDC flows\n",
    "        def P_H_RULE(model, h):\n",
    "            forward_flow_limit = float(self.df_hvdc_links.loc[h, 'FORWARD_LIMIT_MW'])\n",
    "            reverse_flow_limit = float(self.df_hvdc_links.loc[h, 'REVERSE_LIMIT_MW'])\n",
    "            return (- reverse_flow_limit / model.BASE_POWER, forward_flow_limit / model.BASE_POWER)\n",
    "        model.p_H = Var(model.OMEGA_H, bounds=P_H_RULE)\n",
    "\n",
    "        # Node voltage angles\n",
    "        model.theta = Var(model.OMEGA_N)\n",
    "\n",
    "\n",
    "        # Constraints\n",
    "        # -----------\n",
    "        # Power balance\n",
    "        generator_node_map = self.get_generator_node_map(model.OMEGA_G)\n",
    "        network_graph = self.get_network_graph()\n",
    "        def POWER_BALANCE_RULE(model, n):\n",
    "            return (- model.D[n] \n",
    "                    + model.R[n]\n",
    "                    + sum(model.p[g] for g in generator_node_map[n]) \n",
    "                    - sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for m in network_graph[n]) \n",
    "                    - sum(model.K[n, h] * model.p_H[h] for h in model.OMEGA_H) == 0)\n",
    "        model.POWER_BALANCE = Constraint(model.OMEGA_N, rule=POWER_BALANCE_RULE)\n",
    "        \n",
    "        # Max power output\n",
    "        def P_MAX_RULE(model, g):\n",
    "            return model.p[g] <= model.REGISTERED_CAPACITY[g] * model.GENERATION_SHOCK_FACTOR[g]\n",
    "        model.P_MAX = Constraint(model.OMEGA_G, rule=P_MAX_RULE)\n",
    "\n",
    "        # Reference angle\n",
    "        def REFERENCE_ANGLE_RULE(model, n):\n",
    "            if model.S[n] == 1:\n",
    "                return model.theta[n] == 0\n",
    "            else:\n",
    "                return Constraint.Skip\n",
    "        model.REFERENCE_ANGLE = Constraint(model.OMEGA_N, rule=REFERENCE_ANGLE_RULE)\n",
    "\n",
    "        # Voltage angle difference constraint\n",
    "        def VOLTAGE_ANGLE_DIFFERENCE_RULE(model, n, m):\n",
    "            return model.theta[n] - model.theta[m] - model.THETA_DELTA <= 0\n",
    "        model.VOLTAGE_ANGLE_DIFFERENCE = Constraint(model.OMEGA_NM, rule=VOLTAGE_ANGLE_DIFFERENCE_RULE)\n",
    "\n",
    "        # AC interconnector flow constraints\n",
    "        def AC_FLOW_RULE(model, j):\n",
    "            return sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for n, m in ac_limits.loc[j, 'branches'])\n",
    "        model.AC_FLOW = Expression(model.OMEGA_J, rule=AC_FLOW_RULE)\n",
    "\n",
    "        def AC_POWER_FLOW_LIMIT_RULE(model, j):\n",
    "            return model.AC_FLOW[j] - model.F[j] <= 0\n",
    "        model.AC_POWER_FLOW_LIMIT = Constraint(model.OMEGA_J, rule=AC_POWER_FLOW_LIMIT_RULE)\n",
    "\n",
    "        \n",
    "        # Expressions\n",
    "        # -----------\n",
    "        # Effective emissions intensity (original emissions intensity x scaling factor)\n",
    "        def E_HAT_RULE(model, g):\n",
    "            return model.E[g] * model.EMISSIONS_INTENSITY_SHOCK_FACTOR[g]\n",
    "        model.E_HAT = Expression(model.OMEGA_G, rule=E_HAT_RULE)\n",
    "        \n",
    "        # Total emissions [tCO2]\n",
    "        model.TOTAL_EMISSIONS = Expression(expr=sum(model.p[g] * model.E_HAT[g] * model.SCENARIO_DURATION for g in model.OMEGA_G))\n",
    "        \n",
    "        # Total energy output from regulated generators [MWh]\n",
    "        model.TOTAL_REGULATED_GENERATOR_ENERGY = Expression(expr=sum(model.p[g] * model.SCENARIO_DURATION for g in model.OMEGA_G))\n",
    "        \n",
    "        # Average emissions intensity of regulated generators [tCO2/MWh]\n",
    "        model.AVERAGE_EMISSIONS_INTENSITY_REGULATED_GENERATORS = Expression(expr=model.TOTAL_EMISSIONS / model.TOTAL_REGULATED_GENERATOR_ENERGY)\n",
    "        \n",
    "        # Average emissions intensity of system [tCO2/MWh]\n",
    "        model.AVERAGE_EMISSIONS_INTENSITY_SYSTEM = Expression(expr=model.TOTAL_EMISSIONS / model.D)\n",
    "        \n",
    "        # Net scheme revenue [$]\n",
    "        model.NET_SCHEME_REVENUE = Expression(expr=sum(model.E_HAT[g] - model.PHI) * model.p[g] * model.TAU * model.SCENARIO_DURATION for g in model.OMEGA_G)\n",
    "        \n",
    "\n",
    "        # Objective\n",
    "        # ---------\n",
    "        # Cost minimisation\n",
    "        model.OBJECTIVE = Objective(expr=sum((model.C[g] + ((model.E_HAT[g] - model.PHI) * model.TAU)) * model.p[g] for g in model.OMEGA_G))\n",
    "\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def update_model_parameters(self, week_index, scenario_index, baseline, permit_price):\n",
    "        \"\"\" Update DCOPF model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : pyomo object\n",
    "            DCOPF OPF model\n",
    "\n",
    "        df_scenarios : pandas DataFrame\n",
    "            Demand and fixed power injection data for each week and each scenario\n",
    "\n",
    "        week_index : int\n",
    "            Index of week for which model should be run\n",
    "\n",
    "        scenario_index : int\n",
    "            Index of scenario that describes operating condition for the given week\n",
    "\n",
    "        baseline: float\n",
    "            Emissions intensity baseline [tCO2/MWh]\n",
    "\n",
    "        permit price : float\n",
    "            Permit price [$/tCO2]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : pyomo object\n",
    "            DCOPF model object with updated parameters.    \n",
    "        \"\"\"\n",
    "\n",
    "        # Update fixed nodal power injections\n",
    "        for n in self.model.OMEGA_N:\n",
    "            self.model.D[n] = float(self.df_scenarios.loc[('demand', n), (week_index, scenario_index)] / self.model.BASE_POWER.value)\n",
    "            self.model.R[n] = float((self.df_scenarios.loc[('hydro', n), (week_index, scenario_index)] + self.df_scenarios.loc[('intermittent', n), (week_index, scenario_index)]) / self.model.BASE_POWER.value)\n",
    "            \n",
    "        # Update emissions intensity baseline\n",
    "        self.model.PHI = float(baseline)\n",
    "\n",
    "        # Update permit price\n",
    "        self.model.TAU = float(permit_price / self.model.BASE_POWER.value)\n",
    "        \n",
    "        # Scenario duration\n",
    "        self.model.SCENARIO_DURATION = float(self.df_scenarios.loc[('hours', 'duration'), (week_index, scenario_index)])\n",
    "        \n",
    "        # Update week index\n",
    "        self.week_index = week_index\n",
    "        \n",
    "        # Update scenario index\n",
    "        self.scenario_index = scenario_index\n",
    "               \n",
    "            \n",
    "    def solve_model(self):\n",
    "        \"Solve model\"\n",
    "        \n",
    "        self.opt.solve(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from pyomo.environ import *\n",
    "\n",
    "class MPCModel:\n",
    "    \"Model Predictive Controller used to update emissions intensity baseline\"\n",
    "    \n",
    "    def __init__(self, generator_index, forecast_interval_length):\n",
    "        # Create model object\n",
    "        self.model = self.mpc_baseline_updater(generator_index, forecast_interval_length=forecast_interval_length)       \n",
    "                \n",
    "        # Define solver\n",
    "        self.opt = SolverFactory('gurobi', solver_io='lp')\n",
    "\n",
    "        \n",
    "    def mpc_baseline(self, generator_index, forecast_interval_length):\n",
    "        \"\"\"Compute baseline path using model predictive control\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generator_index : list\n",
    "            DUIDs for each generator regulated by the emissions intensity scheme\n",
    "            \n",
    "        forecast_interval_length : int\n",
    "            Number of periods over which the MPC controller has to achieve its objective\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : pyomo model object\n",
    "            Quadratic program used to find path of emissions intensity baseline that\n",
    "            achieves a revenue target while minimising changes to the emissions intensity baseline\n",
    "            over the horizon in which this re-balancing takes place.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise model object\n",
    "        model = ConcreteModel()\n",
    "\n",
    "\n",
    "        # Sets\n",
    "        # ----\n",
    "        # Generators\n",
    "        model.OMEGA_G = Set(initialize=generator_index)\n",
    "\n",
    "        # Time index\n",
    "        model.OMEGA_T = Set(initialize=range(1, forecast_interval_length+1), ordered=True)\n",
    "\n",
    "\n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # Predicted generator emissions intensity for future periods\n",
    "        model.EMISSIONS_INTENSITY_FORECAST = Param(model.OMEGA_G, model.OMEGA_T, initialize=0, mutable=True)\n",
    "\n",
    "        # Predicted weekly energy output\n",
    "        model.ENERGY_FORECAST = Param(model.OMEGA_G, model.OMEGA_T, initialize=0, mutable=True)\n",
    "\n",
    "        # Permit price\n",
    "        model.PERMIT_PRICE = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Emissions intensity baseline from previous period\n",
    "        model.INITIAL_EMISSIONS_INTENSITY_BASELINE = Param(initialize=0, mutable=True)\n",
    "        \n",
    "        # Initial rolling scheme revenue\n",
    "        model.INITIAL_ROLLING_SCHEME_REVENUE = Param(initialize=0, mutable=True)\n",
    "\n",
    "        # Rolling scheme revenue target at end of finite horizon\n",
    "        model.TARGET_ROLLING_SCHEME_REVENUE = Param(initialize=0, mutable=True)\n",
    "\n",
    "\n",
    "        # Variables\n",
    "        # ---------\n",
    "        # Emissions intensity baseline\n",
    "        model.phi = Var(model.OMEGA_T)\n",
    "\n",
    "\n",
    "        # Constraints\n",
    "        # -----------\n",
    "        # Scheme revenue must be at target by end of model horizon\n",
    "        model.SCHEME_REVENUE = Constraint(expr=model.INITIAL_ROLLING_SCHEME_REVENUE + sum((model.EMISSIONS_INTENSITY_FORECAST[g, t] - model.phi[t]) * model.ENERGY_FORECAST[g, t] * model.PERMIT_PRICE for g in model.OMEGA_G for t in model.OMEGA_T) == 0)\n",
    "\n",
    "        # Baseline must be non-negative\n",
    "        def BASELINE_NONNEGATIVE_RULE(model, t):\n",
    "            return model.phi[t] >= 0\n",
    "        model.BASELINE_NONNEGATIVE = Constraint(model.OMEGA_T, rule=BASELINE_NONNEGATIVE_RULE)\n",
    "\n",
    "\n",
    "        # Objective function\n",
    "        # ------------------\n",
    "        # Minimise changes to baseline over finite time horizon\n",
    "        model.OBJECTIVE = Objective(expr=(((model.phi[model.OMEGA_T.first()] - model.INITIAL_EMISSIONS_INTENSITY_BASELINE) * (model.phi[model.OMEGA_T.first()] - model.INITIAL_EMISSIONS_INTENSITY_BASELINE))\n",
    "                                          + sum((model.phi[t] - model.phi[t-1]) * (model.phi[t] - model.phi[t-1]) for t in model.OMEGA_T if t > model.OMEGA_T.first()))\n",
    "                                   )\n",
    "        return model\n",
    "\n",
    "            \n",
    "        def update_model_parameter(self, forecast_generator_emissions_intensity, forecast_generator_energy_output, permit_price, initial_emissions_intensity_baseline, initial_rolling_scheme_revenue, target_rolling_scheme_revenue):\n",
    "            \"\"\"Update parameters used as inputs for the MPC controller\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            forecast_generator_emissions_intensity : dict\n",
    "                Expected generator emissions intensities over forecast interval\n",
    "                \n",
    "            forecast_generator_energy_output : dict\n",
    "                Forecast weekly energy output from generators over the forecast interval      \n",
    "                \n",
    "            permit_price : float\n",
    "                Emissions price [tCO2/MWh]\n",
    "\n",
    "            initial_emissions_intensity_baseline : float\n",
    "                Emissions intensity baseline implemented for preceding week [tCO2/MWh]\n",
    "\n",
    "            initial_rolling_scheme_revenue : float\n",
    "                Rolling scheme revenue at end of preceding week [$]\n",
    "\n",
    "            target_rolling_scheme_revenue : float\n",
    "                Target scheme revenue to be obtained in the future [$]\n",
    "            \"\"\"\n",
    "            \n",
    "            # For each time interval in the forecast horizon\n",
    "            for t in self.model.OMEGA_T:\n",
    "                # For each generator\n",
    "                for g in self.model.OMEGA_G:\n",
    "                    # Predicted generator emissions intensity for future periods\n",
    "                    self.model.EMISSIONS_INTENSITY_FORECAST = float(forecast_generator_emissions_intensity[g][t])\n",
    "\n",
    "                    # Predicted weekly energy output\n",
    "                    self.model.ENERGY_FORECAST = float(forecast_generator_energy_output[g][t])\n",
    "\n",
    "            # Permit price\n",
    "            self.model.PERMIT_PRICE = float(permit_price)\n",
    "            \n",
    "            # Emissions intensity baseline from previous period\n",
    "            self.model.INITIAL_EMISSIONS_INTENSITY_BASELINE = float(initial_emissions_intensity_baseline)\n",
    "\n",
    "            # Initial rolling scheme revenue\n",
    "            self.model.INITIAL_ROLLING_SCHEME_REVENUE = float(initial_rolling_scheme_revenue)\n",
    "\n",
    "            # Rolling scheme revenue target at end of forecast horizon\n",
    "            self.model.TARGET_ROLLING_SCHEME_REVENUE = float(target_rolling_scheme_revenue)\n",
    "            \n",
    "\n",
    "        def solve_model(self):\n",
    "            \"Solve for optimal emissions intensity baseline path\"\n",
    "            \n",
    "            self.opt.solve(self.model)\n",
    "                    \n",
    "                    \n",
    "        def get_optimal_baseline_path(self):\n",
    "            \"Get optimal emissions intenstiy baseline path based on MPC controller\"\n",
    "        \n",
    "            # Optimal emissions intensity baseline path as determined by MPC controller\n",
    "            optimal_baseline_path = OrderedDict(self.model.phi.get_values())\n",
    "            \n",
    "            return optimal_baseline_path\n",
    "            \n",
    "            \n",
    "        def get_next_baseline(self):\n",
    "            \"Get next baseline to be implemented for the coming week\"\n",
    "            \n",
    "            # Optimal path of baselines to be implemented over the finite horizon\n",
    "            optimal_baseline_path = self.get_optimal_baseline_path()\n",
    "            \n",
    "            # Next 'optimal' emissions intensity baseline to implement for the coming interval\n",
    "            next_baseline = float(optimal_baseline_path[self.model.OMEGA_T.first()])\n",
    "\n",
    "            return next_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Paths\n",
    "# -----\n",
    "# Directory containing network and generator data\n",
    "data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, os.path.pardir, 'data')\n",
    "\n",
    "# Path to scenarios directory\n",
    "scenarios_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, '1_create_scenarios', 'output')  \n",
    "\n",
    "\n",
    "def run_model(data_dir, scenarios_dir, shock_option, update_mode, description, week_of_shock=10, initial_permit_price=40, initial_baseline=1, initial_rolling_scheme_revenue=0, seed=10):\n",
    "    \"\"\"Run model with given parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Path to directory containing core data files used in model initialisation\n",
    "\n",
    "    scenarios_dir : str\n",
    "        Path to directory containing representative operating scenarios\n",
    "\n",
    "    shock_option : str\n",
    "        Specifies type of shock to which system will be subjected. Options\n",
    "\n",
    "        Options:\n",
    "            NO_SHOCKS                 - No shocks to system\n",
    "            EMISSIONS_INTENSITY_SHOCK - Emissions intensity scaled by a random number between 0.8 \n",
    "                                        and 1 at 'week_of_shock'\n",
    "    update_mode : str\n",
    "        Specifies how baseline should be updated each week. \n",
    "\n",
    "        Options:\n",
    "            NO_UPDATE                - Same baseline in next iteration\n",
    "            REVENUE_REBALANCE_UPDATE - Recalibrate baseline by trying to re-balance net scheme revenue every interval\n",
    "            \n",
    "    description : str\n",
    "        Description of model / scenario being tested\n",
    "    \n",
    "    week_of_shock : int\n",
    "        Index for week at which either generation or emissions intensities shocks will be implemented / begin.\n",
    "        Default = 10.\n",
    "\n",
    "    target_scheme_revenue : float\n",
    "        Net scheme revenue target [$]. Default = $0.\n",
    "        \n",
    "    initial_permit_price : float\n",
    "        Initialised permit price value [$/tCO2]. Default = 40 $/tCO2.\n",
    "\n",
    "    initial_baseline : float\n",
    "        Initialised emissions intensity baseline value [tCO2/MWh]. Default = 1 tCO2/MWh.\n",
    "\n",
    "    initial_rolling_scheme_revenue : float\n",
    "        Initialised rolling scheme revenue value [$]. Default = $0.\n",
    "\n",
    "    seed : int\n",
    "        Seed used for random number generator. Allows shocks to be reproduced. Default = 10.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    run_id : str\n",
    "        ID used to identify model run\n",
    "    \"\"\"\n",
    "\n",
    "    # Print model being run\n",
    "    print(f'Running model {update_mode} with run option {shock_option}')\n",
    "\n",
    "    # Summary of all parameters defining the model\n",
    "    parameter_values = (shock_option, update_mode, description, week_of_shock, target_scheme_revenue, initial_permit_price, initial_baseline, initial_rolling_scheme_revenue, seed)\n",
    "\n",
    "    # Convert parameters to string\n",
    "    parameter_values_string = ''.join([str(i) for i in parameter_values])\n",
    "\n",
    "    # Find sha256 of parameter values - used as a unique identifier for the model run\n",
    "    run_id = hashlib.sha256(parameter_values_string.encode('utf-8')).hexdigest()[:8].upper()\n",
    "\n",
    "    # Summary of model options, identified by the hash value of these options\n",
    "    run_summary = {run_id: {'shock_option': shock_option, \n",
    "                            'update_mode': update_mode,\n",
    "                            'description': description,\n",
    "                            'week_of_shock': week_of_shock, \n",
    "                            'initial_permit_price': initial_permit_price,\n",
    "                            'initial_baseline': initial_baseline, \n",
    "                            'initial_rolling_scheme_revenue': initial_rolling_scheme_revenue, \n",
    "                            'seed': seed}}\n",
    "\n",
    "\n",
    "    # Check if model parameters valid\n",
    "    # -------------------------------\n",
    "    if update_mode not in ['NO_UPDATE', 'REVENUE_REBALANCE_UPDATE', 'MPC_UPDATE']:\n",
    "        raise Warning(f'Unexpected update_mode encountered: {update_mode}')\n",
    "\n",
    "    if shock_option not in ['NO_SHOCKS', 'EMISSIONS_INTENSITY_SHOCK']:\n",
    "        raise Warning(f'Unexpected shock_option encountered: {shock_option}')\n",
    "\n",
    "\n",
    "    # Create model objects\n",
    "    # --------------------\n",
    "    # Instantiate DCOPF model object\n",
    "    DCOPF = DCOPFModel(data_dir=data_dir, scenarios_dir=scenarios_dir)\n",
    "\n",
    "    # Instantiate Model Predictive Controller\n",
    "    MPC = MPCModel(generator_index=DCOPF.model.OMEGA_G, forecast_interval_length=6)\n",
    "    \n",
    "    \n",
    "    # Result containers\n",
    "    # -----------------\n",
    "    # Prices at each node for each scenario\n",
    "    scenario_nodal_prices = dict()\n",
    "    \n",
    "    # Power output for each scenario\n",
    "    scenario_power_output = dict()\n",
    "\n",
    "    # Results for each scenario\n",
    "    scenario_metrics = {'net_scheme_revenue': dict(),\n",
    "                        'total_emissions_tCO2': dict(),\n",
    "                        'total_regulated_generator_energy_MWh': dict(),\n",
    "                        'total_demand_MWh': dict()\n",
    "                       }\n",
    "    \n",
    "    # Aggregated results for each week\n",
    "    week_metrics = {'baseline': dict(),\n",
    "                    'net_scheme_revenue': dict(),\n",
    "                    'rolling_scheme_revenue': dict(),\n",
    "                    'total_emissions_tCO2': dict(),\n",
    "                    'total_regulated_generator_energy_MWh': dict(),\n",
    "                    'total_demand_MWh': dict(),\n",
    "                    'average_emissions_intensity_regulated_generators': dict(),\n",
    "                    'average_emissions_intensity_system': dict()\n",
    "                   }\n",
    "    \n",
    "    # Random shocks \n",
    "    # -------------\n",
    "    # Set seed so random shocks can be reproduced if needed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Augment original emissions intensity by random scaling factor between 0.8 and 1\n",
    "    if shock_option == 'EMISSIONS_INTENSITY_SHOCK':\n",
    "        df_emissions_intensity_shock_factor = pd.Series(index=DCOPF.model.OMEGA_G, data=np.random.uniform(0.8, 1, len(DCOPF.model.OMEGA_G)))\n",
    "\n",
    "\n",
    "    # Run scenarios\n",
    "    # -------------\n",
    "    # For each week\n",
    "    for week_index in DCOPF.df_scenarios.columns.levels[0]:\n",
    "        # Start clock to see how long it takes to solve all scenarios for each week\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Initialise policy parameters if the first week\n",
    "        if week_index == 1:\n",
    "            # Initialise emissions intensity baseline\n",
    "            baseline = initial_baseline\n",
    "            \n",
    "            # Initialise permit price\n",
    "            permit_price = initial_permit_price\n",
    "            \n",
    "            # Initialise rolling scheme revenue\n",
    "            rolling_scheme_revenue = initial_rolling_scheme_revenue\n",
    "            \n",
    "        \n",
    "        # Record baseline applying for the given week\n",
    "        week_metrics['baseline'][week_index] = baseline\n",
    "\n",
    "        # Apply emissions intensity shock if shock option specified. Shock occurs once at week_index \n",
    "        # given by week_of_shock\n",
    "        if (shock_option == 'EMISSIONS_INTENSITY_SHOCK') and (week_index == week_of_shock):\n",
    "            # Loop through generators\n",
    "            for g in DCOPF.model.OMEGA_G:\n",
    "                # Augement generator emissions intensity factor\n",
    "                DCOPF.model.EMISSIONS_INTENSITY_SHOCK_FACTOR[g] = float(df_emissions_intensity_shock_factor.loc[g])\n",
    "\n",
    "        # For each representative scenario approximating a week's operating state\n",
    "        for scenario_index in DCOPF.df_scenarios.columns.levels[1]:        \n",
    "            # Update model parameters\n",
    "            DCOPF.update_model_parameters(week_index=week_index,\n",
    "                                          scenario_index=scenario_index, \n",
    "                                          baseline=baseline, \n",
    "                                          permit_price=permit_price)\n",
    "\n",
    "            # Solve model\n",
    "            DCOPF.solve_model()\n",
    "\n",
    "\n",
    "            # Store results\n",
    "            # -------------\n",
    "            # Nodal prices\n",
    "            scenario_nodal_prices[(week_index, scenario_index)] = {n: DCOPF.model.dual[DCOPF.model.POWER_BALANCE[n]] for n in DCOPF.model.OMEGA_N}\n",
    "\n",
    "            # Power output from each generator\n",
    "            scenario_power_output[(week_index, scenario_index)] = DCOPF.model.p.get_values()\n",
    "            \n",
    "            \n",
    "            # Store scenario metrics\n",
    "            # ----------------------\n",
    "            # Net scheme revenue for each scenario [$]\n",
    "            scenario_metrics['net_scheme_revenue'][(week_index, scenario_index)] = DCOPF.model.NET_SCHEME_REVENUE.expr()\n",
    "            \n",
    "            # Total emissions [tCO2]\n",
    "            scenario_metrics['total_emissions_tCO2'][(week_index, scenario_index)] = DCOPF.model.TOTAL_EMISSIONS.expr()\n",
    "            \n",
    "            # Total emissions from regulated generators [tCO2]\n",
    "            scenario_metrics['total_regulated_generator_energy_MWh'][(week_index, scenario_index)] = DCOPF.model.TOTAL_REGULATED_GENERATOR_ENERGY.expr()\n",
    "            \n",
    "            # Total system energy demand [MWh]\n",
    "            scenario_metrics['total_demand_MWh'][(week_index, scenario_index)] = DCOPF.model.D.value\n",
    "            \n",
    "            \n",
    "        # Compute aggregate statistics for given week\n",
    "        # -------------------------------------------\n",
    "        # Net scheme revenue [$]\n",
    "        week_metrics['net_scheme_revenue'][week_index] = sum(scenario_metrics['net_scheme_revenue'][(week_index, s)] for s in DCOPF.df_scenarios.columns.levels[1])\n",
    "        \n",
    "        # Total emissions [tCO2]\n",
    "        week_metrics['total_emissions_tCO2'][week_index] = sum(scenario_metrics['total_emissions_tCO2'][(week_index, s)] for s in DCOPF.df_scenarios.columns.levels[1])\n",
    "        \n",
    "        # Total output from generators subjected to emissions intensity policy [MWh]\n",
    "        week_metrics['total_regulated_generator_energy_MWh'][week_index] = sum(scenario_metrics['total_regulated_generator_energy_MWh'][(week_index, s)] for s in DCOPF.df_scenarios.columns.levels[1])\n",
    "        \n",
    "        # Total energy demand in given week [MWh]\n",
    "        week_metrics['total_demand_MWh'][week_index] = sum(scenario_metrics['total_demand_MWh'][(week_index, s)] for s in DCOPF.df_scenarios.columns.levels[1])\n",
    "        \n",
    "        # Average emissions intensity of regulated generators [tCO2/MWh]\n",
    "        week_metrics['average_emissions_intensity_regulated_generators'][week_index] = week_metrics['total_emissions_tCO2'][week_index] / week_metrics['total_regulated_generator_energy_MWh'][week_index]\n",
    "        \n",
    "        # Average emissions intensity of whole system (including renewables) [tCO2/MWh]\n",
    "        week_metrics['average_emissions_intensity_system'][week_index] = week_metrics['total_emissions_tCO2'][week_index] / week_metrics['total_demand_MWh'][week_index]            \n",
    "        \n",
    "        # Update rolling scheme revenue\n",
    "        rolling_scheme_revenue += week_metrics['net_scheme_revenue']\n",
    "\n",
    "        # Record rolling scheme revenue [$]\n",
    "        week_metrics['rolling_scheme_revenue'][week_index] = rolling_scheme_revenue\n",
    "        \n",
    "\n",
    "        # Update baseline\n",
    "        # ---------------\n",
    "        if update_mode == 'NO_UPDATE':\n",
    "            # No update to baseline (should be 0 tCO2/MWh)\n",
    "            baseline = baseline\n",
    "\n",
    "        elif update_mode == 'REVENUE_REBALANCE_UPDATE':\n",
    "            # Update baseline seeking to re-balance net scheme revenue every period\n",
    "            baseline += week_metrics['average_emissions_intensity_regulated_generators'] - ((target_scheme_revenue - rolling_scheme_revenue) / (permit_price * week_metrics['energy_MWh'][week_index]))\n",
    "            \n",
    "            # Set baseline to 0 if updated value less than zero\n",
    "            if baseline < 0:\n",
    "                baseline = 0\n",
    "        \n",
    "        elif update_mode == 'MPC_UPDATE':\n",
    "            # Update baseline using a Model Predictive Control paradigm. Goal is to minimise control \n",
    "            # action (movement of baseline) while achieving target scheme revenue 6 weeks in the future.\n",
    "#             baseline = baseline\n",
    "            \n",
    "            \n",
    "        print(f'Completed week {week_index} in {time.time()-t0:.2f}s')\n",
    "\n",
    "        \n",
    "    # Save results\n",
    "    # ------------\n",
    "    with open(f'output/{run_id}_scenario_nodal_prices.pickle', 'wb') as f:\n",
    "        pickle.dump(scenario_nodal_prices, f)\n",
    "\n",
    "    with open(f'output/{run_id}_scenario_power_output.pickle', 'wb') as f:\n",
    "        pickle.dump(scenario_power_output, f)\n",
    "\n",
    "    with open(f'output/{run_id}_scenario_metrics.pickle', 'wb') as f:\n",
    "        pickle.dump(scenario_results, f)\n",
    "        \n",
    "    with open(f'output/{run_id}_week_metrics.pickle', 'wb') as f:\n",
    "        pickle.dump(week_results, f)\n",
    "        \n",
    "    with open(f'output/{run_id}_run_summary.pickle', 'wb') as f:\n",
    "        pickle.dump(run_summary, f)\n",
    "\n",
    "    with open(f'output/{run_id}_generators.pickle', 'wb') as f:\n",
    "        pickle.dump(DCOPF.df_g, f)\n",
    "        \n",
    "    if shock_option == 'EMISSIONS_INTENSITY_SHOCK':\n",
    "        with open(f'output/{run_id}_emissions_intensity_shock_factor.pickle', 'wb') as f:\n",
    "            pickle.dump(df_emissions_intensity_shock_factor, f)\n",
    "        \n",
    "        \n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model with different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti env",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
