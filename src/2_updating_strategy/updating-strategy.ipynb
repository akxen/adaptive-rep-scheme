{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refunded Emissions Payment Updating Strategy\n",
    "Strategy to update an observable metric, namely the emissions intensity baseline, which is used to augment short-run marginal costs received by generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyomo.environ import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DirectoryPaths(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_dir = os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, 'data')\n",
    "        self.output_dir = os.path.join(os.path.curdir, 'output')\n",
    "\n",
    "paths = DirectoryPaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawData(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Paths to directories\n",
    "        DirectoryPaths.__init__(self)\n",
    "        \n",
    "        \n",
    "        # Network data\n",
    "        # ------------\n",
    "        # Nodes\n",
    "        self.df_n = pd.read_csv(os.path.join(self.data_dir, 'network_nodes.csv'), index_col='NODE_ID')\n",
    "\n",
    "        # AC edges\n",
    "        self.df_e = pd.read_csv(os.path.join(self.data_dir, 'network_edges.csv'), index_col='LINE_ID')\n",
    "\n",
    "        # HVDC links\n",
    "        self.df_hvdc_links = pd.read_csv(os.path.join(self.data_dir, 'network_hvdc_links.csv'), index_col='HVDC_LINK_ID')\n",
    "\n",
    "        # AC interconnector links\n",
    "        self.df_ac_i_links = pd.read_csv(os.path.join(self.data_dir, 'network_ac_interconnector_links.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "        # AC interconnector flow limits\n",
    "        self.df_ac_i_limits = pd.read_csv(os.path.join(self.data_dir, 'network_ac_interconnector_flow_limits.csv'), index_col='INTERCONNECTOR_ID')\n",
    "\n",
    "\n",
    "        # Generators\n",
    "        # ----------       \n",
    "        # Generating unit information\n",
    "        self.df_g = pd.read_csv(os.path.join(self.data_dir, 'generators.csv'), index_col='DUID', dtype={'NODE': int})\n",
    "        self.df_g['SRMC_2016-17'] = self.df_g['SRMC_2016-17'].map(lambda x: x + np.random.uniform(0, 2))\n",
    "        \n",
    "        # Station owners\n",
    "        self.df_g_own = (pd.read_csv(os.path.join(self.data_dir, 'PUBLIC_DVD_STATIONOWNER_201706010000.CSV'), \n",
    "                                    skiprows=1, skipfooter=1, engine='python', parse_dates=['LASTCHANGED'])\n",
    "                         .sort_values('LASTCHANGED', ascending=False)\n",
    "                         .drop_duplicates('STATIONID', keep='first'))\n",
    "\n",
    "        \n",
    "        # Signals\n",
    "        # -------\n",
    "        with open(os.path.join(os.path.curdir, os.path.pardir, '1_create_scenarios', 'output', 'weekly_scenarios.pickle'), 'rb') as f:\n",
    "            self.df_scenarios = pickle.load(f)\n",
    "        \n",
    "        # Sort multi-index\n",
    "        self.df_scenarios.sort_index(ascending=True, inplace=True)\n",
    "        \n",
    "\n",
    "# Create object containing raw model data\n",
    "raw_data = RawData() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OrganiseData(object):\n",
    "    \"Organise data to be used in mathematical program\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load model data\n",
    "        RawData.__init__(self)\n",
    "        \n",
    "        def add_participantids_to_generator_dataframe(self):\n",
    "            \"Add station owner IDs to generators DataFrame\"\n",
    "            \n",
    "            # New generators DataFrame - merging Participant IDs using Station IDs\n",
    "            df_g = (self.df_g.reset_index()\n",
    "                    .merge(self.df_g_own[['PARTICIPANTID', 'STATIONID']], left_on='STATIONID', right_on='STATIONID')\n",
    "                    .set_index('DUID'))\n",
    "            return df_g\n",
    "        \n",
    "        self.df_g = add_participantids_to_generator_dataframe(self)\n",
    "        \n",
    "        def get_aggregate_scenario_energy_demand(self):\n",
    "            \"Regional and national aggregate scenario demand\"\n",
    "\n",
    "            # Total NEM region demand for each operating scenario\n",
    "            scenario_demand = self.df_scenarios.join(pd.concat([self.df_n[['NEM_REGION']]], axis=1, keys=['REGION'])).loc[(slice('demand'), slice(None)), :].groupby(('REGION', 'NEM_REGION')).sum()\n",
    "\n",
    "            # National demand\n",
    "            scenario_demand.loc['NATIONAL'] = scenario_demand.sum()\n",
    "            \n",
    "            # Multiply by duration\n",
    "            scenario_energy_demand = scenario_demand * self.df_scenarios.loc[('hours', 'duration'), :]\n",
    "\n",
    "            return scenario_energy_demand\n",
    "        self.df_aggregate_scenario_energy_demand = get_aggregate_scenario_energy_demand(self)\n",
    "        \n",
    "\n",
    "    def get_admittance_matrix(self):\n",
    "        \"Construct admittance matrix for network\"\n",
    "\n",
    "        # Initialise dataframe\n",
    "        df_Y = pd.DataFrame(data=0j, index=self.df_n.index, columns=self.df_n.index)\n",
    "\n",
    "        # Off-diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, tn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, fn] += - (1 / (row['R_PU'] + 1j * row['X_PU'])) * row['NUM_LINES']\n",
    "\n",
    "        # Diagonal elements\n",
    "        for i in self.df_n.index:\n",
    "            df_Y.loc[i, i] = - df_Y.loc[i, :].sum()\n",
    "\n",
    "        # Add shunt susceptance to diagonal elements\n",
    "        for index, row in self.df_e.iterrows():\n",
    "            fn, tn = row['FROM_NODE'], row['TO_NODE']\n",
    "            df_Y.loc[fn, fn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "            df_Y.loc[tn, tn] += (row['B_PU'] / 2) * row['NUM_LINES']\n",
    "\n",
    "        return df_Y\n",
    "    \n",
    "    \n",
    "    def get_HVDC_incidence_matrix(self):\n",
    "        \"Incidence matrix for HVDC links\"\n",
    "        \n",
    "        # Incidence matrix for HVDC links\n",
    "        df = pd.DataFrame(index=self.df_n.index, columns=self.df_hvdc_links.index, data=0)\n",
    "\n",
    "        for index, row in self.df_hvdc_links.iterrows():\n",
    "            # From nodes assigned a value of 1\n",
    "            df.loc[row['FROM_NODE'], index] = 1\n",
    "\n",
    "            # To nodes assigned a value of -1\n",
    "            df.loc[row['TO_NODE'], index] = -1\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_all_ac_edges(self):\n",
    "        \"Tuples defining from and to nodes for all AC edges (forward and reverse)\"\n",
    "        \n",
    "        # Set of all AC edges\n",
    "        edge_set = set()\n",
    "        \n",
    "        # Loop through edges, add forward and reverse direction indice tuples to set\n",
    "        for index, row in model_data.df_e.iterrows():\n",
    "            edge_set.add((row['FROM_NODE'], row['TO_NODE']))\n",
    "            edge_set.add((row['TO_NODE'], row['FROM_NODE']))\n",
    "        \n",
    "        return edge_set\n",
    "    \n",
    "    def get_network_graph(self):\n",
    "        \"Graph containing connections between all network nodes\"\n",
    "        network_graph = {n: set() for n in model_data.df_n.index}\n",
    "\n",
    "        for index, row in model_data.df_e.iterrows():\n",
    "            network_graph[row['FROM_NODE']].add(row['TO_NODE'])\n",
    "            network_graph[row['TO_NODE']].add(row['FROM_NODE'])\n",
    "        \n",
    "        return network_graph\n",
    "    \n",
    "    \n",
    "    def get_all_dispatchable_fossil_generator_duids(self):\n",
    "        \"Fossil dispatch generator DUIDs\"\n",
    "        \n",
    "        # Filter - keeping only fossil and scheduled generators\n",
    "        mask = (model_data.df_g['FUEL_CAT'] == 'Fossil') & (model_data.df_g['SCHEDULE_TYPE'] == 'SCHEDULED')\n",
    "        \n",
    "        return model_data.df_g[mask].index    \n",
    "    \n",
    "    \n",
    "    def get_intermittent_dispatch(self):\n",
    "        \"Dispatch from intermittent generators (solar, wind)\"\n",
    "        \n",
    "        # Intermittent generator DUIDs\n",
    "        intermittent_duids_mask = model_data.df_g['FUEL_CAT'].isin(['Wind', 'Solar'])\n",
    "        intermittent_duids = model_data.df_g.loc[intermittent_duids_mask].index\n",
    "\n",
    "        # Intermittent dispatch aggregated by node\n",
    "        intermittent_dispatch =(model_data.df_dispatch.reindex(columns=intermittent_duids, fill_value=0)\n",
    "                                .T\n",
    "                                .join(model_data.df_g[['NODE']])\n",
    "                                .groupby('NODE').sum()\n",
    "                                .reindex(index=model_data.df_n.index, fill_value=0)\n",
    "                                .T)\n",
    "        \n",
    "        # Make sure columns are of type datetime\n",
    "        intermittent_dispatch.index = intermittent_dispatch.index.astype('datetime64[ns]')\n",
    "        \n",
    "        return intermittent_dispatch\n",
    "    \n",
    "    \n",
    "    def get_hydro_dispatch(self):\n",
    "        \"Dispatch from hydro plant\"\n",
    "        \n",
    "        # Dispatch from hydro plant\n",
    "        hydro_duids_mask = self.df_g['FUEL_CAT'].isin(['Hydro'])\n",
    "        hydro_duids = self.df_g.loc[hydro_duids_mask].index\n",
    "\n",
    "        # Hydro plant dispatch aggregated by node\n",
    "        hydro_dispatch = (self.df_dispatch.reindex(columns=hydro_duids, fill_value=0)\n",
    "                          .T\n",
    "                          .join(model_data.df_g[['NODE']])\n",
    "                          .groupby('NODE').sum()\n",
    "                          .reindex(index=self.df_n.index, fill_value=0)\n",
    "                          .T)\n",
    "        \n",
    "        # Make sure columns are of type datetime\n",
    "        hydro_dispatch.index = hydro_dispatch.index.astype('datetime64[ns]')\n",
    "        \n",
    "        return hydro_dispatch\n",
    "    \n",
    "    \n",
    "    def get_reference_nodes(self):\n",
    "        \"Get reference node IDs\"\n",
    "        \n",
    "        # Filter Regional Reference Nodes (RRNs) in Tasmania and Victoria.\n",
    "        mask = (model_data.df_n['RRN'] == 1) & (model_data.df_n['NEM_REGION'].isin(['TAS1', 'VIC1']))\n",
    "        reference_node_ids = model_data.df_n[mask].index\n",
    "        \n",
    "        return reference_node_ids\n",
    "    \n",
    "    \n",
    "    def get_node_demand(self):   \n",
    "        \"Compute demand at each node for a given time period, t\"\n",
    "\n",
    "        def _node_demand(row):\n",
    "            # NEM region for a given node\n",
    "            region = row['NEM_REGION']\n",
    "\n",
    "            # Load at node\n",
    "            demand = self.df_load.loc[:, region] * row['PROP_REG_D']\n",
    "\n",
    "            return demand\n",
    "        node_demand = self.df_n.apply(_node_demand, axis=1).T\n",
    "        \n",
    "        return node_demand\n",
    "    \n",
    "    def get_generator_node_map(self, generators):\n",
    "        \"Get set of generators connected to each node\"\n",
    "        generator_node_map = (self.df_g.reindex(index=generators)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={'OMEGA_G': 'DUID'})\n",
    "                              .groupby('NODE').agg(lambda x: set(x))['DUID']\n",
    "                              .reindex(self.df_n.index, fill_value=set()))\n",
    "        \n",
    "        return generator_node_map\n",
    "    \n",
    "    def get_ac_interconnector_summary(self):\n",
    "        \"Summarise aggregate flow limit information for AC interconnectors\"\n",
    "\n",
    "        # Check that from and to regions conform with regional power flow limit directions\n",
    "        def check_flow_direction(row):\n",
    "            if (row['FROM_REGION'] == self.df_ac_i_limits.loc[row.name, 'FROM_REGION']) & (row['TO_REGION'] == model_data.df_ac_i_limits.loc[row.name, 'TO_REGION']):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        # Flow directions are consistent between link and limit DataFrames if True\n",
    "        flow_directions_conform = self.df_ac_i_links.apply(check_flow_direction, axis=1).all()\n",
    "        if flow_directions_conform:\n",
    "            print('Flow directions conform with regional flow limit directions: {0}'.format(flow_directions_conform))\n",
    "        else:\n",
    "            raise(Exception('Link flow directions inconsitent with regional flow forward limit definition'))\n",
    "\n",
    "        # Forward limit\n",
    "        df_forward = self.df_ac_i_links.apply(lambda x: (x['FROM_NODE'], x['TO_NODE']), axis=1).reset_index().groupby('INTERCONNECTOR_ID').agg(lambda x: list(x)).join(model_data.df_ac_i_limits['FORWARD_LIMIT_MW'], how='left').rename(columns={0: 'branches', 'FORWARD_LIMIT_MW': 'limit'})\n",
    "        df_forward['new_index'] = df_forward.apply(lambda x: x.name + '-FORWARD', axis=1)\n",
    "        df_forward.set_index('new_index', inplace=True)\n",
    "\n",
    "        # Reverse limit\n",
    "        df_reverse = self.df_ac_i_links.apply(lambda x: (x['TO_NODE'], x['FROM_NODE']), axis=1).reset_index().groupby('INTERCONNECTOR_ID').agg(lambda x: list(x)).join(model_data.df_ac_i_limits['REVERSE_LIMIT_MW'], how='left').rename(columns={0: 'branches', 'REVERSE_LIMIT_MW': 'limit'})\n",
    "        df_reverse['new_index'] = df_reverse.apply(lambda x: x.name + '-REVERSE', axis=1)\n",
    "        df_reverse.set_index('new_index', inplace=True)\n",
    "        df_ac_limits = pd.concat([df_forward, df_reverse])\n",
    "\n",
    "        return df_ac_limits\n",
    "    \n",
    "# Create object containing organised model data\n",
    "model_data = OrganiseData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(use_pu=False):\n",
    "    \"Create model object\"\n",
    "    \n",
    "    # Initialise model\n",
    "    model = ConcreteModel()\n",
    "\n",
    "    # Sets\n",
    "    # ----   \n",
    "    # Nodes\n",
    "    model.OMEGA_N = Set(initialize=model_data.df_n.index)\n",
    "\n",
    "    # Generators\n",
    "    model.OMEGA_G = Set(initialize=model_data.get_all_dispatchable_fossil_generator_duids())\n",
    "\n",
    "    # AC edges\n",
    "    ac_edges = model_data.get_all_ac_edges()\n",
    "    model.OMEGA_NM = Set(initialize=ac_edges)\n",
    "    \n",
    "    # Sets of branches for which aggregate AC interconnector limits are defined\n",
    "    ac_limits = model_data.get_ac_interconnector_summary()\n",
    "    model.OMEGA_J = Set(initialize=ac_limits.index)\n",
    "    \n",
    "    # HVDC links\n",
    "    model.OMEGA_H = Set(initialize=model_data.df_hvdc_links.index)\n",
    "\n",
    "\n",
    "    # Parameters\n",
    "    # ----------\n",
    "    # System base power\n",
    "    model.BASE_POWER = Param(initialize=100)\n",
    "    \n",
    "    # Emissions intensity baseline\n",
    "    model.PHI = Param(initialize=0, mutable=True)\n",
    "    \n",
    "    # Permit price\n",
    "    model.TAU = Param(initialize=0, mutable=True)\n",
    "    \n",
    "    # Generator emissions intensities\n",
    "    def E_RULE(model, g):\n",
    "        return float(model_data.df_g.loc[g, 'EMISSIONS'])\n",
    "    model.E = Param(model.OMEGA_G, rule=E_RULE)\n",
    "    \n",
    "    # Admittance matrix\n",
    "    admittance_matrix = model_data.get_admittance_matrix()\n",
    "    def B_RULE(model, n, m):\n",
    "        if use_pu:\n",
    "            return float(np.imag(admittance_matrix.loc[n, m]))\n",
    "        else:\n",
    "            return model.BASE_POWER * float(np.imag(admittance_matrix.loc[n, m]))\n",
    "    model.B = Param(model.OMEGA_NM, rule=B_RULE)\n",
    "\n",
    "    # Reference nodes\n",
    "    reference_nodes = model_data.get_reference_nodes()\n",
    "    def S_RULE(model, n):\n",
    "        if n in reference_nodes:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    model.S = Param(model.OMEGA_N, rule=S_RULE)\n",
    "\n",
    "    # Generator short-run marginal costs\n",
    "    def C_RULE(model, g):\n",
    "        marginal_cost = float(model_data.df_g.loc[g, 'SRMC_2016-17'])\n",
    "        if use_pu:\n",
    "            return marginal_cost / model.BASE_POWER\n",
    "        else:\n",
    "            return marginal_cost\n",
    "    model.C = Param(model.OMEGA_G, rule=C_RULE)\n",
    "\n",
    "    # Demand\n",
    "    model.D = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "    \n",
    "    # Max voltage angle difference between connected nodes\n",
    "    model.THETA_DELTA = Param(initialize=float(pi / 2))\n",
    "    \n",
    "    # HVDC incidence matrix\n",
    "    hvdc_incidence_matrix = model_data.get_HVDC_incidence_matrix()\n",
    "    def K_RULE(model, n, h):\n",
    "        return float(hvdc_incidence_matrix.loc[n, h])\n",
    "    model.K = Param(model.OMEGA_N, model.OMEGA_H, rule=K_RULE)    \n",
    "    \n",
    "    # Aggregate AC interconnector flow limits\n",
    "    def F_RULE(model, j):\n",
    "        power_flow_limit = float(ac_limits.loc[j, 'limit'])\n",
    "        if use_pu:\n",
    "            return power_flow_limit / model.BASE_POWER\n",
    "        else:\n",
    "            return power_flow_limit\n",
    "    model.F = Param(model.OMEGA_J, rule=F_RULE)\n",
    "    \n",
    "    # Fixed power injections\n",
    "    model.R = Param(model.OMEGA_N, initialize=0, mutable=True)\n",
    "    \n",
    "    \n",
    "    # Variables\n",
    "    # ---------\n",
    "    # Generator output\n",
    "    def P_RULE(model, g):\n",
    "        registered_capacity = float(model_data.df_g.loc[g, 'REG_CAP'])\n",
    "        if use_pu:\n",
    "            return (0, registered_capacity / model.BASE_POWER)\n",
    "        else:\n",
    "            return (0, registered_capacity)\n",
    "    model.p = Var(model.OMEGA_G, bounds=P_RULE)\n",
    "\n",
    "    # HVDC flows\n",
    "    def P_H_RULE(model, h):\n",
    "        forward_flow_limit = float(model_data.df_hvdc_links.loc[h, 'FORWARD_LIMIT_MW'])\n",
    "        reverse_flow_limit = float(model_data.df_hvdc_links.loc[h, 'REVERSE_LIMIT_MW'])\n",
    "        if use_pu:\n",
    "            return (- reverse_flow_limit / model.BASE_POWER, forward_flow_limit / model.BASE_POWER)\n",
    "        else:\n",
    "            return (- reverse_flow_limit, forward_flow_limit)\n",
    "    model.p_H = Var(model.OMEGA_H, bounds=P_H_RULE)\n",
    "    \n",
    "    # Node voltage angles\n",
    "    model.theta = Var(model.OMEGA_N)\n",
    "\n",
    "\n",
    "    # Constraints\n",
    "    # -----------\n",
    "    # Power balance\n",
    "    generator_node_map = model_data.get_generator_node_map(model.OMEGA_G)\n",
    "    network_graph = model_data.get_network_graph()\n",
    "    def POWER_BALANCE_RULE(model, n):\n",
    "        return (- model.D[n] \n",
    "                + model.R[n]\n",
    "                + sum(model.p[g] for g in generator_node_map[n]) \n",
    "                - sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for m in network_graph[n]) \n",
    "                - sum(model.K[n, h] * model.p_H[h] for h in model.OMEGA_H) == 0)\n",
    "    model.POWER_BALANCE = Constraint(model.OMEGA_N, rule=POWER_BALANCE_RULE)\n",
    "\n",
    "    # Reference angle\n",
    "    def REFERENCE_ANGLE_RULE(model, n):\n",
    "        if model.S[n] == 1:\n",
    "            return model.theta[n] == 0\n",
    "        else:\n",
    "            return Constraint.Skip\n",
    "    model.REFERENCE_ANGLE = Constraint(model.OMEGA_N, rule=REFERENCE_ANGLE_RULE)\n",
    "\n",
    "    # Voltage angle difference constraint\n",
    "    def VOLTAGE_ANGLE_DIFFERENCE_RULE(model, n, m):\n",
    "        return model.theta[n] - model.theta[m] - model.THETA_DELTA <= 0\n",
    "    model.VOLTAGE_ANGLE_DIFFERENCE = Constraint(model.OMEGA_NM, rule=VOLTAGE_ANGLE_DIFFERENCE_RULE)\n",
    "    \n",
    "    # AC interconnector flow constraints\n",
    "    def AC_FLOW_RULE(model, j):\n",
    "        return sum(model.B[n, m] * (model.theta[n] - model.theta[m]) for n, m in ac_limits.loc[j, 'branches'])\n",
    "    model.AC_FLOW = Expression(model.OMEGA_J, rule=AC_FLOW_RULE)\n",
    "    \n",
    "    def AC_POWER_FLOW_LIMIT_RULE(model, j):\n",
    "        return model.AC_FLOW[j] - model.F[j] <= 0\n",
    "    model.AC_POWER_FLOW_LIMIT = Constraint(model.OMEGA_J, rule=AC_POWER_FLOW_LIMIT_RULE)\n",
    "    \n",
    "\n",
    "    # Objective\n",
    "    # ---------\n",
    "    model.x_1 = Var(model.OMEGA_J, within=NonNegativeReals)\n",
    "    model.x_2 = Var(model.OMEGA_J, within=NonNegativeReals)\n",
    "    \n",
    "    def ABS_HVDC_FLOW_1_RULE(model, j):\n",
    "        return model.x_1[j] >= model.AC_FLOW[j]\n",
    "    model.ABS_HVDC_FLOW_1 = Constraint(model.OMEGA_J, rule=ABS_HVDC_FLOW_1_RULE)\n",
    "    \n",
    "    def ABS_HVDC_FLOW_2_RULE(model, j):\n",
    "        return model.x_2[j] >= - model.AC_FLOW[j]\n",
    "    model.ABS_HVDC_FLOW_2 = Constraint(model.OMEGA_J, rule=ABS_HVDC_FLOW_2_RULE)\n",
    "    \n",
    "    def HVDC_FLOW_COST_RULE(model):\n",
    "        if use_pu:\n",
    "            return 10 / model.BASE_POWER\n",
    "        else:\n",
    "            return 10\n",
    "    model.HVDC_FLOW_COST = Param(initialize=HVDC_FLOW_COST_RULE)\n",
    "    \n",
    "    model.OBJECTIVE = Objective(expr=sum((model.C[g] + ((model.E[g] - model.PHI) * model.TAU)) * model.p[g] for g in model.OMEGA_G) + sum((model.x_1[j] + model.x_2[j]) * model.HVDC_FLOW_COST for j in model.OMEGA_J))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow directions conform with regional flow limit directions: True\n"
     ]
    }
   ],
   "source": [
    "model = create_model(use_pu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup solver\n",
    "# ------------\n",
    "solver = 'gurobi'\n",
    "solver_io = 'lp'\n",
    "model.dual = Suffix(direction=Suffix.IMPORT)\n",
    "opt = SolverFactory(solver, solver_io=solver_io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters and solve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_scenario(model, use_pu, permit_price, baseline, week, scenario, tee=False):\n",
    "    \"Update demand and fixed power injection parameters and solve model\"\n",
    "    \n",
    "    # Update permit price\n",
    "    if use_pu:\n",
    "        model.TAU = permit_price / model.BASE_POWER.value\n",
    "    else:\n",
    "        model.TAU = permit_price\n",
    "    \n",
    "    # Update emissions intensity baseline\n",
    "    model.PHI = baseline\n",
    "    \n",
    "    # Update parameters for each node\n",
    "    for n in model.OMEGA_N:\n",
    "        # Node demand\n",
    "        node_demand = model_data.df_scenarios.loc[('demand', n), (week, scenario)]\n",
    "\n",
    "        # Hydro power injection\n",
    "        node_hydro = model_data.df_scenarios.loc[('hydro', n), (week, scenario)]\n",
    "\n",
    "        # Intermittent power injection\n",
    "        node_intermittent = model_data.df_scenarios.loc[('intermittent', n), (week, scenario)]\n",
    "\n",
    "        # If using per-unit scaling\n",
    "        if use_pu:\n",
    "            model.D[n] = node_demand / model.BASE_POWER.value\n",
    "            model.R[n] = (node_hydro + node_intermittent) / model.BASE_POWER.value\n",
    "        else:\n",
    "            model.D[n] = node_demand\n",
    "            model.R[n] = node_hydro + node_intermittent\n",
    "    \n",
    "    # Solve model\n",
    "    res = opt.solve(model, keepfiles=False, tee=tee, warmstart=False)\n",
    "    model.solutions.store_to(res)\n",
    "\n",
    "    # Place results in DataFrame\n",
    "    df = pd.DataFrame(res['Solution'][0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_average_electricity_price(df, use_pu, week, scenario):\n",
    "    \"\"\"Compute average electricity price given a results DataFrame\n",
    "    \n",
    "    Param\n",
    "    -----\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing model results\n",
    "        \n",
    "    use_pu : bool\n",
    "        Indicator if p.u. normalisation employed\n",
    "        \n",
    "    week : int\n",
    "        Week number\n",
    "        \n",
    "    scenario : int\n",
    "        Scenario number\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_average_prices : pandas DataFrame\n",
    "        DataFrame containing average regional and national wholesale electricity prices    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter power-balance results\n",
    "    mask = df.index.str.contains('POWER_BALANCE')\n",
    "    \n",
    "    # Filtered DataFrame\n",
    "    df_f = df.loc[mask].copy()\n",
    "\n",
    "    # Extract node IDs\n",
    "    df_f['node'] = df_f.apply(lambda x: int(re.findall(r'\\[(\\d+)\\]', x.name)[0]), axis=1)\n",
    "\n",
    "    # Dual variable value\n",
    "    df_f['value'] = df_f.apply(lambda x: x['Constraint']['Dual'], axis=1)\n",
    "\n",
    "    # Check if price normalisation has been used\n",
    "    if df_f['value'].max() > 10:\n",
    "        raise(Warning('Per-unit normalisation probably not be used. May need to re-scale'))\n",
    "\n",
    "    # Join demand data\n",
    "    df_f['demand'] = df_f.apply(lambda x: model_data.df_scenarios.loc[('demand', x['node']), (week, scenario)], axis=1)\n",
    "\n",
    "    # Join NEM regions\n",
    "    df_f = df_f.merge(model_data.df_n[['NEM_REGION']], left_on='node', right_index=True, how='left')\n",
    "\n",
    "    # Prices for each NEM region\n",
    "    df_average_prices = df_f.groupby('NEM_REGION').apply(lambda x: x['value'].mul(x['demand']).mul(100).sum() / x['demand'].sum())\n",
    "\n",
    "    # Add national average price\n",
    "    df_average_prices.loc['NATIONAL'] = df_f['value'].mul(df_f['demand']).mul(100).sum() / df_f['demand'].sum()\n",
    "    \n",
    "    # Total revenue (national)\n",
    "    total_revenue = df_f.apply(lambda x: x['value'] * x['demand'] * 100, axis=1).sum()\n",
    "    \n",
    "    # Total demand (national)\n",
    "    total_demand = df_f['demand'].sum()\n",
    "\n",
    "    return {'average_price': df_average_prices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scenario_revenue_and_emissions(df, baseline, permit_price, week, scenario):\n",
    "    \"Get total net REP scheme revenue and emissions for a given operating scenario\"\n",
    "    \n",
    "    # Filter elements that correspond to power output\n",
    "    mask = df.index.str.contains('p\\[')\n",
    "    df_out = df[mask].copy()\n",
    "\n",
    "    # Extract DUIDs\n",
    "    df_out['DUID'] = df_out.apply(lambda x: re.findall(r'p\\[(.+)\\]', x.name)[0], axis=1)\n",
    "\n",
    "    # Comput power output (scale by 100 to account for p.u. normalisation)\n",
    "    df_out['value'] = df_out.apply(lambda x: x['Variable']['Value'] * 100, axis=1)\n",
    "\n",
    "    # Join emissions intensities and marginal costs\n",
    "    df_out = df_out.merge(model_data.df_g[['SRMC_2016-17', 'EMISSIONS', 'NEM_REGION']], left_on='DUID', right_index=True, how='left')\n",
    "\n",
    "    # Scenario duration (hours)\n",
    "    scenario_duration = model_data.df_scenarios.loc[('hours', 'duration'), (week, scenario)]\n",
    "\n",
    "    # Regional and national scenario revenue\n",
    "    scenario_revenue = df_out.groupby('NEM_REGION').apply(lambda x: (x['EMISSIONS'] - baseline) * permit_price * x['value']).reset_index().groupby('NEM_REGION')[0].sum().mul(scenario_duration)\n",
    "    scenario_revenue.loc['NATIONAL'] = scenario_revenue.sum()\n",
    "    \n",
    "    # Regional and national scenario emissions\n",
    "    scenario_emissions = df_out.groupby('NEM_REGION').apply(lambda x: x['EMISSIONS'] * x['value']).reset_index().groupby('NEM_REGION')[0].sum().mul(scenario_duration)\n",
    "    scenario_emissions.loc['NATIONAL'] = scenario_emissions.sum()\n",
    "    \n",
    "    # Regional and national scenario emissions intensity\n",
    "    scenario_emissions_intensity = scenario_emissions / model_data.df_aggregate_scenario_energy_demand.loc[:, (week, scenario)]\n",
    "\n",
    "    return {'scenario_rep_revenue': scenario_revenue, 'scenario_emissions': scenario_emissions, 'scenario_emissions_intensity': scenario_emissions_intensity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Weeks to loop through\n",
    "# weeks = range(1, 53)\n",
    "\n",
    "# # Scenario indices to loop through\n",
    "# scenarios = range(1, 11)\n",
    "\n",
    "# # Container for scenario results for each week\n",
    "# scenario_results = dict()\n",
    "\n",
    "# # Container for summarised weekly results\n",
    "# weekly_summary = dict()\n",
    "\n",
    "# # Revenue target\n",
    "# revenue_target = 0\n",
    "\n",
    "# # Set permit price\n",
    "# permit_price = 40\n",
    "\n",
    "# for gain in np.linspace(0, 1, 9):\n",
    "#     print('Testing gain {0}'.format(gain))\n",
    "#     # Rolling revenue, updated each iteration, initialised to 0 for each gain scenario\n",
    "#     revenue_rolling = 0\n",
    "    \n",
    "#     for week in weeks:\n",
    "#         # Start clock for iteration time\n",
    "#         time_start = time.time()\n",
    "\n",
    "#         # Initialise results dictionary\n",
    "#         scenario_results[week] = dict()\n",
    "\n",
    "#         # Set emissions intensity baseline if in first period\n",
    "#         if week == 1:\n",
    "#             baseline = 1\n",
    "\n",
    "#         # Loop through operating scenarios for each week\n",
    "#         for scenario in scenarios:\n",
    "#             # Solve model\n",
    "#             df = solve_scenario(model=model, use_pu=True, permit_price=permit_price, baseline=baseline, week=week, scenario=scenario)\n",
    "\n",
    "#             # Get revenue and emissions results\n",
    "#             scenario_revenue_and_emissions = get_scenario_revenue_and_emissions(df, baseline=baseline, permit_price=permit_price, week=week, scenario=scenario)\n",
    "\n",
    "#             # Get average prices\n",
    "#             scenario_average_electricity_price = get_average_electricity_price(df, use_pu=True, week=week, scenario=scenario)\n",
    "\n",
    "#             # Save results in dictionary\n",
    "#             scenario_results[week][scenario] = {**scenario_revenue_and_emissions, **scenario_average_electricity_price, 'gain': gain}\n",
    "\n",
    "#         # End-of-week net revenue from REP payments\n",
    "#         revenue_net_end_of_week = sum(scenario_results[week][scenario]['scenario_rep_revenue'].loc['NATIONAL'] for scenario in scenarios)\n",
    "\n",
    "#         # Rolling scheme revenue (total)\n",
    "#         revenue_rolling += revenue_net_end_of_week\n",
    "\n",
    "#         # Difference between total revenue and target\n",
    "#         revenue_difference = revenue_target - revenue_rolling\n",
    "\n",
    "#         # End-of-week total energy demand\n",
    "#         total_energy_demand_end_of_week = model_data.df_aggregate_scenario_energy_demand.loc['NATIONAL', (week, slice(None))].sum()\n",
    "\n",
    "#         # End-of-week total emissions\n",
    "#         emissions_end_of_week = sum(scenario_results[week][scenario]['scenario_emissions'].loc['NATIONAL'] for scenario in scenarios)\n",
    "\n",
    "#         # End-of-week average emissions intensity\n",
    "#         emissions_intensity_end_of_week = emissions_end_of_week / total_energy_demand_end_of_week\n",
    "\n",
    "#         # Store scenario results in dictionary\n",
    "#         weekly_summary[week] = {'baseline': baseline,\n",
    "#                                 'revenue_net_end_of_week': revenue_net_end_of_week,\n",
    "#                                 'revenue_rolling': revenue_rolling,\n",
    "#                                 'revenue_difference': revenue_difference,\n",
    "#                                 'total_energy_demand_end_of_week': total_energy_demand_end_of_week,\n",
    "#                                 'emissions_end_of_week': emissions_end_of_week,\n",
    "#                                 'emissions_intensity_end_of_week': emissions_intensity_end_of_week,\n",
    "#                                 'gain': gain}\n",
    "\n",
    "#         # Update emissions intensity baseline for following week\n",
    "#     #     baseline = emissions_intensity_end_of_week - ((gain * revenue_difference) / (permit_price * total_energy_demand_end_of_week))\n",
    "#     #     baseline = 1 - ((gain * revenue_difference) / (permit_price * total_energy_demand_end_of_week))\n",
    "#         baseline += - ((gain * revenue_difference) / (permit_price * total_energy_demand_end_of_week))\n",
    "\n",
    "#         # Set lower-bound on emissions intensity baseline\n",
    "#         if baseline < 0:\n",
    "#             baseline = 0\n",
    "\n",
    "#         print('Week {0} completed in {1}s.'.format(week, time.time() - time_start))\n",
    "\n",
    "#     scenario_results_filename = 'scenario_results_gain_{0:.3f}.pickle'.format(gain)\n",
    "#     with open(os.path.join(paths.output_dir, scenario_results_filename), 'wb') as f:\n",
    "#         pickle.dump(scenario_results, f)\n",
    "\n",
    "#     weekly_summary_filename = 'weekly_summary_gain_{0:.3f}.pickle'.format(gain)\n",
    "#     with open(os.path.join(paths.output_dir, weekly_summary_filename), 'wb') as g:\n",
    "#         pickle.dump(weekly_summary, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_policy_scenario(model, gain, permit_price, baseline_initial, revenue_initial, revenue_target, policy_scenario_type):\n",
    "    \"\"\"Run scenario using specified permit prices, gains, baselines, and revenue targets\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model : Pyomo object\n",
    "        Instantiated Pyomo object for which parameters will be updated in each scenario\n",
    "    \n",
    "    gain : float\n",
    "        Gain to be used when updating emissions intensity baseline based on difference\n",
    "        between rolling and target revenue.\n",
    "        \n",
    "    permit_price : float\n",
    "        Emissions permit price [$/tCO2]\n",
    "        \n",
    "    baseline_initial : float\n",
    "        Emissions intensity baseline to use during first week of model run\n",
    "        \n",
    "    revenue_initial : float\n",
    "        Initial scheme revenue endowment [$]\n",
    "        \n",
    "    revenue_target : float\n",
    "        Target scheme revenue [$]\n",
    "    \n",
    "    policy_scenario_type : str\n",
    "        Indentifier for type of policy analysis being investigated. String will appear\n",
    "        in file names    \n",
    "    \"\"\"\n",
    "       \n",
    "    # String summarising parameters used in policy scenario analysis\n",
    "    test_overview_string = \"\"\"\n",
    "    Policy Scenario Overview\n",
    "    ------------------------\n",
    "    gain: {0}\n",
    "    permit price: {1}\n",
    "    baseline initial: {2}\n",
    "    revenue_initial: {3}\n",
    "    revenue_target: {4}\n",
    "    policy_scenario_type: {5}    \n",
    "    \"\"\".format(gain, permit_price, baseline_initial, revenue_initial, revenue_target, policy_scenario_type)\n",
    "    print(test_overview_string)\n",
    "    \n",
    "    # Weeks to loop through\n",
    "    weeks = range(1, 53)\n",
    "\n",
    "    # Scenario indices to loop through\n",
    "    scenarios = range(1, 11)\n",
    "\n",
    "    # Container for scenario results for each week\n",
    "    scenario_results = dict()\n",
    "\n",
    "    # Container for summarised weekly results\n",
    "    weekly_summary = dict()\n",
    "    \n",
    "    # Rolling revenue, updated each iteration, initialised to 0 for each gain scenario\n",
    "    revenue_rolling = revenue_initial\n",
    "    \n",
    "    # Loop through weeks\n",
    "    for week in weeks:\n",
    "        # Start clock for iteration time\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Initialise results dictionary\n",
    "        scenario_results[week] = dict()\n",
    "\n",
    "        # Set emissions intensity baseline if in first period\n",
    "        if week == 1:\n",
    "            baseline = baseline_initial\n",
    "\n",
    "        # Loop through operating scenarios for each week\n",
    "        for scenario in scenarios:\n",
    "            # Solve model\n",
    "            df = solve_scenario(model=model, use_pu=True, permit_price=permit_price, baseline=baseline, week=week, scenario=scenario)\n",
    "\n",
    "            # Get revenue and emissions results\n",
    "            scenario_revenue_and_emissions = get_scenario_revenue_and_emissions(df, baseline=baseline, permit_price=permit_price, week=week, scenario=scenario)\n",
    "\n",
    "            # Get average prices\n",
    "            scenario_average_electricity_price = get_average_electricity_price(df, use_pu=True, week=week, scenario=scenario)\n",
    "\n",
    "            # Save results in dictionary\n",
    "            scenario_results[week][scenario] = {**scenario_revenue_and_emissions,\n",
    "                                                **scenario_average_electricity_price, \n",
    "                                                'gain': gain,\n",
    "                                                'permit_price': permit_price,\n",
    "                                                'baseline_initial': baseline_initial, \n",
    "                                                'revenue_initial': revenue_initial, \n",
    "                                                'revenue_target': revenue_target, \n",
    "                                                'policy_scenario_type': policy_scenario_type}\n",
    "\n",
    "        # End-of-week net revenue from REP payments\n",
    "        revenue_net_end_of_week = sum(scenario_results[week][scenario]['scenario_rep_revenue'].loc['NATIONAL'] for scenario in scenarios)\n",
    "\n",
    "        # Rolling scheme revenue (total)\n",
    "        revenue_rolling += revenue_net_end_of_week\n",
    "\n",
    "        # Difference between total revenue and target\n",
    "        revenue_difference = revenue_target - revenue_rolling\n",
    "\n",
    "        # End-of-week total energy demand\n",
    "        total_energy_demand_end_of_week = model_data.df_aggregate_scenario_energy_demand.loc['NATIONAL', (week, slice(None))].sum()\n",
    "\n",
    "        # End-of-week total emissions\n",
    "        emissions_end_of_week = sum(scenario_results[week][scenario]['scenario_emissions'].loc['NATIONAL'] for scenario in scenarios)\n",
    "\n",
    "        # End-of-week average emissions intensity\n",
    "        emissions_intensity_end_of_week = emissions_end_of_week / total_energy_demand_end_of_week\n",
    "\n",
    "        # Store scenario results in dictionary\n",
    "        weekly_summary[week] = {'baseline': baseline,\n",
    "                                'revenue_net_end_of_week': revenue_net_end_of_week,\n",
    "                                'revenue_rolling': revenue_rolling,\n",
    "                                'revenue_difference': revenue_difference,\n",
    "                                'total_energy_demand_end_of_week': total_energy_demand_end_of_week,\n",
    "                                'emissions_end_of_week': emissions_end_of_week,\n",
    "                                'emissions_intensity_end_of_week': emissions_intensity_end_of_week,\n",
    "                                'gain': gain,\n",
    "                                'permit_price': permit_price,\n",
    "                                'baseline_initial': baseline_initial,\n",
    "                                'revenue_initial': revenue_initial,\n",
    "                                'revenue_target': revenue_target,\n",
    "                                'policy_scenario_type': policy_scenario_type}\n",
    "\n",
    "        # Update emissions intensity baseline for following week\n",
    "        if permit_price == 0:\n",
    "            # If permit price is 0 (i.e. business-as-usual scenario) to prevent baseline from being undefined\n",
    "            baseline = 0\n",
    "        else:\n",
    "            baseline += - ((gain * revenue_difference) / (permit_price * total_energy_demand_end_of_week))\n",
    "\n",
    "        # Set lower-bound on emissions intensity baseline\n",
    "        if baseline < 0:\n",
    "            baseline = 0\n",
    "\n",
    "        print('Week {0} completed in {1}s.'.format(week, time.time() - time_start))\n",
    "\n",
    "    # Construct filename using policy scenario type string as an identifer, as well as a timestamp\n",
    "    scenario_results_filename = 'scenario_results_{0}_{1}.pickle'.format(policy_scenario_type, int(time.time()))\n",
    "    with open(os.path.join(paths.output_dir, scenario_results_filename), 'wb') as f:\n",
    "        pickle.dump(scenario_results, f)\n",
    "\n",
    "    # Construct filename using policy scenario type string as an identifer, as well as a timestamp\n",
    "    weekly_summary_filename = 'weekly_summary_{0}_{1}.pickle'.format(policy_scenario_type, int(time.time()))\n",
    "    with open(os.path.join(paths.output_dir, weekly_summary_filename), 'wb') as g:\n",
    "        pickle.dump(weekly_summary, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run different policy scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Policy Scenario Overview\n",
      "    ------------------------\n",
      "    gain: 0\n",
      "    permit price: 0\n",
      "    baseline initial: 0\n",
      "    revenue_initial: 0\n",
      "    revenue_target: 0\n",
      "    policy_scenario_type: bau    \n",
      "    \n",
      "Week 1 completed in 15.59314250946045s.\n",
      "Week 2 completed in 15.699368238449097s.\n",
      "Week 3 completed in 15.452215433120728s.\n",
      "Week 4 completed in 15.605666160583496s.\n",
      "Week 5 completed in 15.68372368812561s.\n",
      "Week 6 completed in 16.03827667236328s.\n",
      "Week 7 completed in 16.869693279266357s.\n",
      "Week 8 completed in 16.35588002204895s.\n",
      "Week 9 completed in 16.434004545211792s.\n",
      "Week 10 completed in 16.06119418144226s.\n",
      "Week 11 completed in 16.25787377357483s.\n",
      "Week 12 completed in 15.57667875289917s.\n",
      "Week 13 completed in 15.730666160583496s.\n",
      "Week 14 completed in 15.45422077178955s.\n",
      "Week 15 completed in 15.527531862258911s.\n",
      "Week 16 completed in 15.557259798049927s.\n",
      "Week 17 completed in 15.906280040740967s.\n",
      "Week 18 completed in 15.511945009231567s.\n",
      "Week 19 completed in 15.809748649597168s.\n",
      "Week 20 completed in 15.66818618774414s.\n",
      "Week 21 completed in 16.052143812179565s.\n",
      "Week 22 completed in 15.789669275283813s.\n",
      "Week 23 completed in 16.011489391326904s.\n",
      "Week 24 completed in 15.73131775856018s.\n",
      "Week 25 completed in 15.966765642166138s.\n",
      "Week 26 completed in 15.68811583518982s.\n",
      "Week 27 completed in 15.605676412582397s.\n",
      "Week 28 completed in 15.762286186218262s.\n",
      "Week 29 completed in 15.71501898765564s.\n",
      "Week 30 completed in 15.636911630630493s.\n",
      "Week 31 completed in 15.614573955535889s.\n",
      "Week 32 completed in 15.574450016021729s.\n",
      "Week 33 completed in 15.577181577682495s.\n",
      "Week 34 completed in 15.476434707641602s.\n",
      "Week 35 completed in 15.60567021369934s.\n",
      "Week 36 completed in 15.684442043304443s.\n",
      "Week 37 completed in 15.57439136505127s.\n",
      "Week 38 completed in 15.878342151641846s.\n",
      "Week 39 completed in 16.540203332901s.\n",
      "Week 40 completed in 17.093607187271118s.\n",
      "Week 41 completed in 18.138054370880127s.\n",
      "Week 42 completed in 17.05014395713806s.\n",
      "Week 43 completed in 17.329701900482178s.\n",
      "Week 44 completed in 17.24291467666626s.\n",
      "Week 45 completed in 17.33709692955017s.\n",
      "Week 46 completed in 17.49014186859131s.\n",
      "Week 47 completed in 17.249823093414307s.\n",
      "Week 48 completed in 17.239880323410034s.\n",
      "Week 49 completed in 16.95169949531555s.\n",
      "Week 50 completed in 17.068195581436157s.\n",
      "Week 51 completed in 17.320086002349854s.\n",
      "Week 52 completed in 16.888184785842896s.\n"
     ]
    }
   ],
   "source": [
    "# Baseline model\n",
    "run_policy_scenario(model=model, gain=0, permit_price=0, baseline_initial=0, revenue_initial=0, revenue_target=0, policy_scenario_type='bau')\n",
    "\n",
    "# # Gain sensitivity analysis\n",
    "# for gain in np.linspace(0, 1, 9):\n",
    "#     run_policy_scenario(model=model, gain=gain, permit_price=40, baseline_initial=1, revenue_initial=0, revenue_target=0, policy_scenario_type='gain_sensitivity_analysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti env",
   "language": "python",
   "name": "opti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
